{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDyiu4nje7Y6"
   },
   "source": [
    "# Labeling for Pixel-wise Classification with Sentinel-2 Satellite Imagery\n",
    "\n",
    "Michael Mommert, Stuttgart University of Applied Sciences, 2025\n",
    "\n",
    "This Notebook introduces the process of data annotation or labeling for a subsequent classification task. You will learn how to label satellite images with a web-based tool and how to prepare the resulting data for a pixel-wise classification task. We will showcase the process for a [tiny Sentinel-2 sample dataset](https://zenodo.org/records/12819787). For more details on the supervised learning techniques used in this Notebook, please refer to the Notebook [*Pixel-wise Classification with Machine Learning Methods for Sentinel-2 Satellite Imagery*](https://github.com/Hochschule-fuer-Technik-Stuttgart/teaching-mommert/blob/main/classification/pixel-wise/ml/sentinel-2/classification_pixel-wise_ml_sentinel2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU5aylRme7Y_"
   },
   "outputs": [],
   "source": [
    "%pip install numpy \\\n",
    "    scipy \\\n",
    "    shapely \\\n",
    "    matplotlib \\\n",
    "    rasterio \\\n",
    "    seaborn \\\n",
    "    scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IJ7a8tQe7ZB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.features import rasterize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METyrzvye7ZC"
   },
   "source": [
    "## Data Download and Preprocessing\n",
    "\n",
    "We download the Sentinel-2 sample dataset used in this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuXqy5yNe7ZC"
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!wget https://zenodo.org/records/14990200/files/sentinel2.zip?download=1 -O sentinel2.zip\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# extract dataset zipfile\n",
    "with zipfile.ZipFile('sentinel2.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ1gXRJZe7ZD"
   },
   "source": [
    "The dataset contains 5 different scenes in a coastal setup. Each image contains all 12 Sentinel-2 Level-2A bands.\n",
    "\n",
    "Due to the small size of the dataset, we can read in the entire dataset into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrFfTJcIe7ZE"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "filenames = sorted(os.listdir('data/'))\n",
    "for filename in filenames:\n",
    "    if filename.endswith('.npy'):\n",
    "        data.append(np.load(open(os.path.join('data', filename), 'rb'), allow_pickle=True))\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj2Zx3aWe7ZF"
   },
   "source": [
    "The data is now stored as a Numpy array, following the shape convention `[scene, band, height, width]`.\n",
    "\n",
    "Let's display one of the images. In order to do so, we have to do two things:\n",
    "\n",
    "1. we have to change the shape to `[height, width, bands]` (this particular shape is expected by matplotlib)\n",
    "2. we have to normalize the pixel values (which vary on a large range) to a range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roDVzHtWe7ZH"
   },
   "outputs": [],
   "source": [
    "i = 1  # image id\n",
    "\n",
    "# first, we extract the R, G and B bands and stack them into the shape [120, 120, 3]\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "\n",
    "# then we normalize the pixel values in such a way that they range from 0 (min) to 1 (max)\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "\n",
    "# now we can plot the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP4J7pUoe7ZJ"
   },
   "source": [
    "Our goal is now to label different land cover classes in our dataset. Potential classes are `water`, `forest`, `grassland` and `sand`, all of which are present in the image shown above. Labeling means to assign image regions to those different classes. We could define these areas within Python, but that is cumbersome.\n",
    "\n",
    "Instead, we will use a web-based tool for generating the labels. In order to use this tool, we have to create simple image files (such as `.png`) which the tool can read in. Since `.png` files can only store RGB data, we have to extract these bands and save the resulting image as `.png` files. One more thing to pay attention to is that the resulting image file should have the same dimensions as the original data (120 x 120 pixels); If this is not enforced, the resulting labels must be transformed to the correct image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtdGELcne7ZJ"
   },
   "outputs": [],
   "source": [
    "# create a `pngs` directory, if it not yet exists\n",
    "os.mkdir('pngs') if not os.path.exists('pngs/') else None\n",
    "\n",
    "# loop over all images in the dataset\n",
    "png_filenames = []\n",
    "for i in range(len(data)):\n",
    "\n",
    "    # extract RGB bands and normalize\n",
    "    img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "    img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(5, 5))  # create an image canvas of a fixed size (5 inches x 5 inches)\n",
    "    ax.imshow(img)  # plot image\n",
    "    plt.axis('off')  # remove axes labels\n",
    "    plt.tight_layout(pad=0)  # remove padding around the image\n",
    "    png_filenames.append('img_{:03d}.png'.format(i))\n",
    "    plt.savefig(os.path.join('pngs', png_filenames[-1]), dpi=img.shape[0]/5)  # write file; define dpi value to force correct image size\n",
    "    plt.close()  # close plot to clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaxFN4Xve7ZK"
   },
   "source": [
    "The images can now be found in the `pngs/` directory. Before we can start with the labeling process, you have to **download** the `pngs/` directory to your local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOGSYfDie7ZL"
   },
   "source": [
    "## Labeling\n",
    "\n",
    "We will use the [*ImgLab*](https://solothought.com/imglab/) web-tool to perform the labeling. *ImgLab* is rather simplistic, but it offers all the functionality that we will need in the following. Other tools, such as [*Label Studio*](https://labelstud.io/) offer more functionality and convenience, but they require local installation. For large-scale labeling campaigns, I would definitely recommend *Label Studio*, but for the purpose of this tutorial, *ImgLab*'s browser-based app is easier to use.\n",
    "\n",
    "Follow these steps:\n",
    "1. Open [*ImgLab*](https://solothought.com/imglab/) in your browser.\n",
    "2. Click on the **folder symbol** in the bottom left corner. This will allow you to **import images from a folder**. Select the `pngs/` directory that you downloaded. Once the images have been imported, you see the five images on the bottom of the screen.\n",
    "3. Click on the first image. It will be displayed in the main area of the screen. Since our images are rather small (120 x 120 pixels), it makes sense to zoom in. Use the **zoom function** in the bottom left corner of the screen. Click on the magnifying glass. The magnifying factor will appear at the top of the screen. Increase the magnification until the image details are easy to see for you.\n",
    "4. We begin the labeling of the first image. Click on the **Polygon symbol** on the left; your cursor will turn into a crosshair. Pick an area that you would like to label and create polygon nodes by following and clickking on its outline. Once your polygon is complete, hit the **Enter key**. If you made a mistake and would like to remove the polygon, simply click on it and hit the Delete key. If the polygon is fine, click on it and **select a category name** in the top right corner of the screen. This will assign a class name to the polygon; simply type in the name. Repeat this step to label a number of areas in the image and in the other images. Make sure to use consistent class names.\n",
    "5. Once you're done with labeling, you can **export the labels**. Different formats are available. For our purposes, please download the labels as **COCO JSON**.\n",
    "6. Finally, please upload the resulting `.json` file to your Notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8KQhSyre7ZL"
   },
   "source": [
    "## Label processing\n",
    "\n",
    "In the following, you can use your own label file. Simply replace the filename in the next code cell. Alternatively, you can use a pre-built label file called `coastal_labels.json`.\n",
    "\n",
    "Let's have a look at the `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_dro-Qse7ZL"
   },
   "outputs": [],
   "source": [
    "rawlabels = json.load(open('coastal_labels.json', 'r'))\n",
    "rawlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLuNsO3we7ZM"
   },
   "source": [
    "The file contains a lot of information. Let's have a look at the main attributes, which are stored as the keys of the resulting dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpDRBpgle7ZM"
   },
   "outputs": [],
   "source": [
    "rawlabels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Xb9ZNpe7ZN"
   },
   "source": [
    "What do those attributes mean?\n",
    "\n",
    "* `images` contains the list of image filenames used in the labeling. For each image, it contains its filename, dimensions and an id number.\n",
    "* `types` defines the type of labels; in our case, we provide instance labels (each instance is labeled separately).\n",
    "* `annotations` is the most important attribute and contains a list of polygons that you created. For each polygon, it contains a list of the node coordinates (`segmentation`), the `image_id`, class id (`category_id`) and other attributes.\n",
    "* `categories` lists the different classes that are available. For each class, it contains the name (what you provided in the labeling process), a unique id number and a supercategory (which we don't use here).\n",
    "\n",
    "Let's extract one polygon and reassemble it using the `shapely` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3PRxi05e7ZO"
   },
   "outputs": [],
   "source": [
    "coordsraw = rawlabels['annotations'][0]['segmentation'][0]  # extract raw coordinate list (x_0, y_0, x_1, y_1, x_2...)\n",
    "coords = [(coordsraw[i], coordsraw[i+1]) for i in range(0, len(coordsraw), 2)]  # split coordinates by x and y\n",
    "Polygon(coords)  # turn coordinates into Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq7RIBcSe7ZO"
   },
   "source": [
    "This looks like a polygon. Let's extract all polygons from one of the images and plot them on the image.\n",
    "\n",
    "But before we do so, let's assemble the different class names and assign colors to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g_I2KhZe7ZP"
   },
   "outputs": [],
   "source": [
    "# extract class names\n",
    "class_names = {}\n",
    "for c in rawlabels['categories']:\n",
    "    class_names[c['id']] =  c['name']\n",
    "\n",
    "# define class colors (RGB values)\n",
    "class_colors = np.array([\n",
    "    (0, 0, 0), # background should be black\n",
    "    (0, 0, 1),  # class 1 (water)\n",
    "    (1, 1, 0.8),  # class 2 (sand)\n",
    "    (0.2, 0.8, 0.2),  # class 3 (grassland)\n",
    "    (0.1, 0.5,0.1)])  # class 4 (forest)\n",
    "class_cmap_nobackground = mpl.colors.ListedColormap(class_colors[1:])\n",
    "class_cmap = mpl.colors.ListedColormap(class_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIuw0KoAQ3Ib"
   },
   "source": [
    "Now we plot an image with the corresponding polygon labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxyJW7Nse7ZP"
   },
   "outputs": [],
   "source": [
    "i = 2 # image index\n",
    "\n",
    "# identify image_id used in rawlabels for this image\n",
    "filename = png_filenames[i]\n",
    "image_id = None\n",
    "for imgfile in rawlabels['images']:\n",
    "    if imgfile['file_name'] == filename:\n",
    "        image_id = imgfile['id']\n",
    "\n",
    "# extract RGB bands and normalize, plot image\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "plt.imshow(img)\n",
    "\n",
    "# identify annotations that correspond to this image\n",
    "for j in range(len(rawlabels['annotations'])):\n",
    "    if rawlabels['annotations'][j]['image_id'] == image_id:\n",
    "        # extract coordinates and class\n",
    "        coordsraw = rawlabels['annotations'][j]['segmentation'][0]\n",
    "        coords = np.array([(coordsraw[m], coordsraw[m+1]) for m in range(0, len(coordsraw), 2)])\n",
    "        class_id = rawlabels['annotations'][j]['category_id']\n",
    "\n",
    "        # plot polygon based on coordinates\n",
    "        plt.fill(*coords.transpose(), color=class_colors[class_id], label=class_names[class_id], edgecolor='black', linewidth=2, alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFFChb5Te7ZQ"
   },
   "source": [
    "This looks good. Now let's turn this into masks for each image and each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suRfjhm4e7ZR"
   },
   "outputs": [],
   "source": [
    "# store labels as array with shape [scene, height, width]\n",
    "# note that we add a background class for those image areas that are not labeled\n",
    "labels = np.zeros((len(data), data.shape[-2], data.shape[-1]))\n",
    "\n",
    "# for each image...\n",
    "for i in range(len(data)):\n",
    "\n",
    "    # extract image data\n",
    "    imgdata = data[i]\n",
    "\n",
    "    # identify image_id used in rawlabels for this image\n",
    "    filename = png_filenames[i]\n",
    "    image_id = None\n",
    "    for imgfile in rawlabels['images']:\n",
    "        if imgfile['file_name'] == filename:\n",
    "            image_id = imgfile['id']\n",
    "\n",
    "    # identify annotations that correspond to this image\n",
    "    for j in range(len(rawlabels['annotations'])):\n",
    "        if rawlabels['annotations'][j]['image_id'] == image_id:\n",
    "            # extract coordinates and class\n",
    "            coordsraw = rawlabels['annotations'][j]['segmentation'][0]\n",
    "            coords = np.array([(coordsraw[m], coordsraw[m+1]) for m in range(0, len(coordsraw), 2)])\n",
    "            class_id = rawlabels['annotations'][j]['category_id']\n",
    "\n",
    "            if len(coords) < 3:\n",
    "                # if there's less than 3 points, it's not a polygon\n",
    "                continue\n",
    "\n",
    "            # create a polygon and rasterize it\n",
    "            polygon = Polygon(coords)\n",
    "            m = rasterize([polygon], out_shape=(120, 120))\n",
    "            labels[i] = labels[i] + m*class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab9nhX4Fe7ZR"
   },
   "source": [
    "Now we generated masks that show us which of the labeled pixels belong to which class. Let's have a look at one of the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkupH4ije7ZR"
   },
   "outputs": [],
   "source": [
    "i = 2 # image index\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "# extract RGB bands and normalize, plot image\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "ax[0].imshow(img)\n",
    "\n",
    "# plot segmentation mask\n",
    "ax[1].imshow(labels[i], cmap=class_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdLU8Alce7ZS"
   },
   "source": [
    "Note the fact that the vast majority of pixels is black - that is the case since all of those pixels are not labeled. We have to be careful to only consider those pixels in the training process that have labels. Let's extract the labeled pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvHm4G-Ee7ZT"
   },
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for c in range(1, len(class_names)+1):\n",
    "    for i in range(len(data)):\n",
    "        _X = np.dstack(data[i])[labels[i] == c]  # extract spectral properties\n",
    "        _y = np.array([c for _ in range(len(_X))])  # extract class index\n",
    "        # append results\n",
    "        if len(X) == 0:\n",
    "            X = _X\n",
    "            y = _y\n",
    "        else:\n",
    "            X = np.concatenate([X, _X], axis=0)\n",
    "            y = np.concatenate([y, _y], axis=0)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L80n64Ame7ZT"
   },
   "source": [
    "Great, now we have a table of labeled pixels with corresponding classes. Before we can use the data, we have to split them into train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn2l7U4ce7ZU"
   },
   "outputs": [],
   "source": [
    "# we split the entire dataset into a training (70%) and remain (30%) split; the remain fraction will be split into validation (50%) and test (50%)\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remain, y_remain, train_size=0.5, shuffle=True, random_state=42, stratify=y_remain)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoCceUbZe7ZU"
   },
   "source": [
    "## k-Nearest Neighbor Classification\n",
    "\n",
    "Now we can use a k-NN (or any other classifier) to classify our dataset. We use $k=5$ - but this is just a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlRXrStVe7ZV"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate the model\n",
    "model = KNeighborsClassifier(5)\n",
    "\n",
    "# \"train\" the model on the training dataset\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r1343I7e7ZV"
   },
   "source": [
    "As we did before, we plot the prediction for the entire scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YemsJDSJe7ZW"
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "# predict classes for each pixel\n",
    "pred = model.predict(np.dstack(data[i]).reshape(-1, 12))\n",
    "\n",
    "f, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(12, 6))\n",
    "\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])  # we extract the R, G, B bands for this scene\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "ax[0].imshow(img)\n",
    "\n",
    "ax[1].imshow(pred.reshape(120, 120), cmap=class_cmap_nobackground)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYnXauzAe7ZW"
   },
   "source": [
    "The qualitative result looks very good! What about the accuracy metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MhZvCjDe7ZW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgVUmMsPe7ZY"
   },
   "source": [
    "This looks also very good.\n",
    "\n",
    "We can look at the mistakes the model makes using a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc1hEKBce7ZY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=list(class_names.values())[:-1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9Z0ifg-e7ZZ"
   },
   "source": [
    "There is very little confusion between the classes. In fact, the only confusion is between the grassland and forest classes, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKHpVXQ9e7ZZ"
   },
   "source": [
    "**Exercise**: Label more polygons and train a model based on the combined dataset. Will the accuracy improve even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b38Hfo3e7ZZ"
   },
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
