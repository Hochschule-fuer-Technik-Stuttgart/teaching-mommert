{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0038ff-bbc6-4d00-a86f-7934b2ff6ecd",
   "metadata": {},
   "source": [
    "# Unsupervised Segmentation for Sentinel-2 Satellite Imagery\n",
    "\n",
    "Michael Mommert, Stuttgart University of Applied Sciences, 2024\n",
    "\n",
    "This Notebook introduces unsupervised segmentation as a task. We will apply segmentation to a set of 5 Sentinel-2 satellite images (extracted from the [BigEarthNet dataset](https://bigearth.net/)). We will implement and test different segmentation methods to identify image areas with (hopefully) similar semantic meaning. \n",
    "\n",
    "Keep in mind that we treat segmentation as an unsupervised method here: there are no class labels involved. We will introduce supervised classification (also on segmented images as we produce here) in a pixel-wise classification tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb85d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d33ee-e5ae-4022-bd47-0f88fa41df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b7c70e",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "We will download a small sample dataset containing Sentinel-2 satellite imagery and unpack the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7151128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!wget https://zenodo.org/records/12819787/files/sentinel2_coastal_scenes.zip?download=1 -O sentinel2.zip\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# extract dataset zipfile\n",
    "with zipfile.ZipFile('sentinel2.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f017ad",
   "metadata": {},
   "source": [
    "We read in all images from the dataset into a single NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f35d5-0cf5-4787-81b5-348496410189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for filename in os.listdir('data/'):\n",
    "    if filename.endswith('.npy'):\n",
    "        data.append(np.load(open(os.path.join('data', filename), 'rb'), allow_pickle=True))\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13c3ea",
   "metadata": {},
   "source": [
    "The `data` array is built up in such a way that it contains 5 images, 12 bands per image and each image has a height of 120 pixels and a width of 120 pixels. This dimensionality is reflected by the shape of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c564158-8cd2-495a-ab87-939a3199ffe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91196eb4",
   "metadata": {},
   "source": [
    "**Exercise**: Experiment with the data array and extract the following information:\n",
    "\n",
    "* all bands of the second image;\n",
    "* the green band (index 2) of the third image;\n",
    "* RGB bands (indices 3, 2 and 1) for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff187c9-c131-4e32-9f8b-218b067104a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a85e7b",
   "metadata": {},
   "source": [
    "Let's visualize one of the images. We will introduce a variable `i` that refers to the image index (possible values: 0, 1, 2, 3, 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3a3ec-f12d-4ae5-9e25-5c851c6cdc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0  # image id\n",
    "\n",
    "# first, we extract the R, G and B bands and stack them into the shape [120, 120, 3]\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "\n",
    "# then we normalize the pixel values in such a way that they range from 0 (min) to 1 (max)\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "\n",
    "# now we can plot the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388add3b",
   "metadata": {},
   "source": [
    "**Exercise**: Familiarize yourself with the different images in the dataset by changing `i` in the previous code cell. What land cover types do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c3f7f-c487-4187-845f-b6048e109e79",
   "metadata": {},
   "source": [
    "## Segmentation with RGB data\n",
    "\n",
    "We start segmentation on RGB-only data. For that purpose, we extract a `data_rgb` array that only contains the RGB bands (in the order B, G and R; the order is not important for the segmentation process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1501e1-981f-47ba-a293-f559103d4131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_rgb = data[:, 1:4, :, :]\n",
    "data_rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710798a",
   "metadata": {},
   "source": [
    "Our method of choice for segmentation is a clustering method, $k$-Means. This method gets as input a list of samples and it will find *clusters* or overdensities, i.e., areas in this sample space where many samples can be found. \n",
    "\n",
    "In this application, each sample is represented by a pixel from our input image (or images). Using only the RGB image data, each sample is represented by a 3d vector. Clusters therefore represent sets of pixels with similar spectral properties. Hence, we can use this technique as a means to perform image segmentation. \n",
    "\n",
    "We apply k-Means clustering to segment our RGB dataset. We have to set the number of clusters ($k$) that we would like to identify. We will start with two clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bf10c-81b1-4645-b154-80270dc96520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 2 # number of clusters\n",
    "\n",
    "model = KMeans(n_clusters=k)  # instantiate the method, which we call our \"model\"\n",
    "\n",
    "# we \"fit\" the model to the first image; this means that it finds clusters based on the pixels in that image\n",
    "model.fit(np.dstack(data_rgb[0]).reshape(-1, data_rgb.shape[1]))\n",
    "# note that we have to linearize the image: every pixel (rows) is represented by 12 bands (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1602ab",
   "metadata": {},
   "source": [
    "Now that the model is fit, we can \"predict\" the cluster assignment for every pixel in a given image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedc9ee-47ca-459d-b520-abcce897f4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "i = 3  # image id\n",
    "\n",
    "# this is the prediction step that assigns every pixel a cluster id\n",
    "pred = model.predict(np.dstack(data_rgb[i]).reshape(-1, data_rgb.shape[1])).reshape(data_rgb[i].shape[1:])\n",
    "# Note that we have to linearize the input data again, and we have to reshape the output in the original image shape \n",
    "\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])  # extract RGB\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))  # normalize\n",
    "\n",
    "# plot the image with an overlay indicating the different segments\n",
    "plt.imshow(mark_boundaries(img, pred, color=(255, 255, 255), mode='subpixel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a61d8",
   "metadata": {},
   "source": [
    "**Exercise**: Describe the segmentation output. Are the output segments large or small? How do you think we can improve the segmentation result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95969f53-fd5b-453d-83ad-99490078ebd5",
   "metadata": {},
   "source": [
    "## Segmentation with multispectral data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7d185",
   "metadata": {},
   "source": [
    "We repeat the same k-Means segmentation ($k=2$) with multispectral data (all 12 bands instead of 3): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ceb40-0f4e-49ba-a8cc-1489c3a507c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "i = 2  # image id\n",
    "\n",
    "# we \"fit\" our model to the first image\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(np.dstack(data[0]).reshape(-1, data.shape[1]))\n",
    "\n",
    "# we predict results for image i\n",
    "pred = model.predict(np.dstack(data[i]).reshape(-1, data.shape[1])).reshape(data[i].shape[1:])\n",
    "\n",
    "# we plot the results \n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "plt.imshow(mark_boundaries(img, pred, color=(255, 255, 255), mode='subpixel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab9456",
   "metadata": {},
   "source": [
    "**Exercise**: Are the predictions derived by this model better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb84a95-0fd9-4da9-bfdd-44b4b3ca2f99",
   "metadata": {},
   "source": [
    "**Exercise**: Identify the band or bands that are necessary for a robust identification of surface waters. (Hint: copy code cell above and play with the band combinations that go into the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4189dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445cf27-9fdc-49a9-a44a-dd5781a55eaa",
   "metadata": {},
   "source": [
    "## More data...\n",
    "\n",
    "So far, we have only applied segmentation to individual images. Now we will apply the clustering to our entire dataset (using all available bands and all available images). Let's see how the results will vary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd7ecb-b44a-46bb-8e65-3427f517f30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 4  # image id\n",
    "k = 2  # number of clusters\n",
    "\n",
    "# fit model to all images and all bands\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(np.stack(np.dstack(data)).reshape(-1, 12))\n",
    "\n",
    "# predict cluster affiliation for image i\n",
    "pred = model.predict(np.dstack(data[i]).reshape(-1, 12)).reshape(data[i].shape[1:])\n",
    " \n",
    "# plot results\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "plt.imshow(mark_boundaries(img, pred, color=(255, 255, 255), mode='subpixel'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b0201-be40-4ddf-9093-2a145d6ff365",
   "metadata": {},
   "source": [
    "**Exercise**: What do you observe? Did the segmentation results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a84230-720c-4bd3-ae38-76181010c715",
   "metadata": {},
   "source": [
    "## More clusters...\n",
    "\n",
    "So far, we have only considered two clusters. But we are not limited to two clusters. Let's increase the number of clusters to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da158826-0aeb-4e5b-887b-b40acdbdabbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0  # image id\n",
    "k = 5  # number of clusters\n",
    "\n",
    "# fit model to all images and all bands\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(np.stack(np.dstack(data)).reshape(-1, 12))\n",
    "\n",
    "# predict cluster affiliation for image i\n",
    "pred = model.predict(np.dstack(data[i]).reshape(-1, 12)).reshape(data[i].shape[1:])\n",
    " \n",
    "# plot results\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "plt.imshow(mark_boundaries(img, pred, color=(255, 255, 255), mode='subpixel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee25847",
   "metadata": {},
   "source": [
    "The result looks crowded. Let's have a look at the individual clusters that were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8aad8-bd7e-4862-b476-3cca2bdd593e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 5  # number of clusters\n",
    "i = 0   # image id\n",
    "\n",
    "# extract RGB data for plotting\n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "\n",
    "# fit model to all images and all bands\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(np.dstack(data).reshape(-1, 12))\n",
    "\n",
    "# predict cluster affiliation for image i\n",
    "pred = model.predict(np.dstack(data[i]).reshape(-1, 12)).reshape(data[i].shape[1:])\n",
    "\n",
    "# create plot\n",
    "f, ax = plt.subplots(1, k+2, sharex=True, sharey=True, figsize=(15, 3))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# plot the image \n",
    "ax[0].imshow(img)\n",
    "ax[0].axis('off')\n",
    "\n",
    "# plot the image with boundaries\n",
    "ax[1].imshow(mark_boundaries(img, pred, color=(255, 255, 255)))\n",
    "ax[1].axis('off')\n",
    "\n",
    "# plot the individual clusters\n",
    "for i in range(k):\n",
    "    tmp_img = img.copy()  # create a copy of img\n",
    "    tmp_img[pred != i] = (0, 0, 0)  # set those pixels to zero that are not part of cluster i\n",
    "    ax[i+2].imshow(tmp_img)  # plot this cluster\n",
    "    ax[i+2].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b5b1e-c0c1-47d4-a770-fc5520ecd7ad",
   "metadata": {},
   "source": [
    "**Exercise**: explore the impact of the number of classes on the segmentation results. Which number of clusters works best? What pseudo-labels would you assign to the different classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0068df-0900-467c-bcfa-4d1f52a5ec79",
   "metadata": {},
   "source": [
    "## SLIC Segmentation\n",
    "\n",
    "We will now use a different segmentation method. SLIC, or Simple Linear Iterative Clustering, builds on the $k$-Means algorithm to perform clustering. However, instead of only considering the pixel values in the different bands, it performs clustering on a 5-dimensional input: a tranformation of RGB colors and the two pixel coordinates. \n",
    "\n",
    "SLIC results in *superpixels*, contiguous pixel areas with similar spectral properties. The goal is that all pixels in the same superpixel have similar spectral properties and that all superpixels are approximately of the same size. The idea is that these superpixels can be used as input for a subsequent supervised classification.\n",
    "\n",
    "Let's generate some superpixels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac2859-a783-46f4-a699-dacebfa17bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "\n",
    "i = 0 # image id\n",
    "\n",
    "# apply SLIC to (RGB-only data of) image i; the n_segments parameter controls the approximate number of superpixels to be defines\n",
    "pred_slic = slic(np.dstack(data_rgb[i]), n_segments=40, start_label=0)\n",
    "\n",
    "# extract and normalize RGB data for plotting \n",
    "img = np.dstack([data[i][3], data[i][2], data[i][1]])\n",
    "img = (img-np.min(img, axis=(0,1)))/(np.max(img, axis=(0,1)) - np.min(img, axis=(0,1)))\n",
    "\n",
    "# plot results\n",
    "plt.imshow(mark_boundaries(img, pred_slic, color=(255, 255, 255), mode='subpixel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb9de9-fde4-4e94-a182-e39595b486f4",
   "metadata": {},
   "source": [
    "**Exercise**: play with the parameters of the SLIC method. What settings derive superpixels that look promising for a subsequent classification process based on land-use/land-cover classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be56760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
