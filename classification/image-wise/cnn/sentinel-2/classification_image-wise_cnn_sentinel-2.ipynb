{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification with a Convolutional Neural Network for Sentinel-2 Satellite Imagery\n",
    "\n",
    "Michael Mommert, Stuttgart University of Applied Sciences, 2025\n",
    "\n",
    "This Notebook showcases how a Convolutional Neural Network can be used to perform image classification. This task will be trained on image labels corresponding to the most prevalent land-use/land-cover class in Sentinel-2 satellite images from the [*ben-ge-800* dataset](https://zenodo.org/records/12941231). This Notebook builds on top of the [Pixel-wise Classification with a Multilayer Perceptron](https://github.com/Hochschule-fuer-Technik-Stuttgart/teaching-mommert/blob/main/classification/pixel-wise/mlp/sentinel-2/classification_pixel-wise_mlp_sentinel-2.ipynb) Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy \\\n",
    "    scipy \\\n",
    "    pandas \\\n",
    "    matplotlib \\\n",
    "    rasterio \\\n",
    "    scikit-learn \\\n",
    "    torch \\\n",
    "    torchmetrics \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Download\n",
    "\n",
    "We're setting up our Python environment for this tutorial by installing and importing the necessary modules and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi5NrxiUYT_m"
   },
   "outputs": [],
   "source": [
    "# system level modules for handling files and file structures\n",
    "import os\n",
    "import tarfile\n",
    "import copy\n",
    "\n",
    "# scipy ecosystem imports for numerics, data handling and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# pytorch and helper modules\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# rasterio for reading in satellite image data\n",
    "import rasterio as rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We download the *ben-ge-800* dataset and unpack it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://zenodo.org/records/12941231/files/ben-ge-800.tar.gz?download=1 -O ben-ge-800.tar.gz\n",
    "  \n",
    "#tarfile = tarfile.open('ben-ge-800.tar.gz')  # open ben-ge-800 tarball \n",
    "#tarfile.extractall('./', filter='data')  # extract tarball\n",
    "\n",
    "#data_base_path = os.path.join(os.path.abspath('.'), 'ben-ge-800')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvTJ9WPQOxon"
   },
   "source": [
    "**ben-ge-800** contains samples for 800 locations with co-located Sentinel-1 SAR data, Sentinel-2 multispectral data, elevation data, land-use/land-cover data, as well as environmental data. **ben-ge-800** is a subset of the much larger **ben-ge** dataset (see [https://github.com/HSG-AIML/ben-ge](https://github.com/HSG-AIML/ben-ge) for details.) We deliberately use a very small subset of **ben-ge** to enable reasonable runtimes for the examples shown in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yver4nem5anJ"
   },
   "source": [
    "The environment is now set up and the data in place. Before we define the dataset classes and dataloaders to access the data efficiently, we fix some random seeds to obtain reproduceable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOZcKSCT5anJ"
   },
   "outputs": [],
   "source": [
    "data_base_path = os.path.join(os.path.abspath('.'), 'ben-ge-800')\n",
    "\n",
    "np.random.seed(42)     # sets the seed value in Numpy\n",
    "torch.manual_seed(42)  # sets the seed value in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4UnJK1N5anJ"
   },
   "source": [
    "## Data Handling\n",
    "\n",
    "Before we start implementing our model, let's have a look at the data. In this notebook, we need two different data products that are available for every single sample in the dataset:\n",
    "* Sentinel-2 multispectral data: 12-band Level-2A images of size 120x120; we will restrict ourselves to the 4 bands that carry 10m-resolution imaging data (bands 2, 3, 4 and 8)\n",
    "* [ESAWorldCover](https://esa-worldcover.org/en) land-use/land-cover image labels: for each image, this label consists of the most prevalent (based on area covered in the image) land-use/land-cover class in the image.\n",
    "\n",
    "We will train a image classification model to predict this land-use/land-cover label for each image. \n",
    "\n",
    "For this purpose, we modify our dataset class from the Notebook [Supervised Classification with Machine Learning Methods for Sentinel-2 Satellite Imagery](https://github.com/Hochschule-fuer-Technik-Stuttgart/teaching-mommert/blob/main/remote_sensing/classification/lulc_ml/lulc_ml.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdVdNb2qP7OQ"
   },
   "outputs": [],
   "source": [
    "# define labels of the different lulc labels\n",
    "ewc_label_names = [\"tree_cover\", \"shrubland\", \"grassland\", \"cropland\", \"built-up\",\n",
    "                   \"bare/sparse_vegetation\", \"snow_and_ice\",\"permanent_water_bodies\",\n",
    "                   \"herbaceous_wetland\", \"mangroves\",\"moss_and_lichen\"]\n",
    "\n",
    "class BENGE(Dataset):\n",
    "    \"\"\"A dataset class implementing the Sentinel-1, Sentinel-2 and ESAWorldCover data modalities.\"\"\"\n",
    "    def __init__(self, \n",
    "                 data_dir=data_base_path, \n",
    "                 split='train',\n",
    "                 s2_bands=[2, 3, 4, 8]):\n",
    "        \"\"\"Dataset class constructor\n",
    "\n",
    "        keyword arguments:\n",
    "        data_dir -- string containing the path to the base directory of ben-ge dataset, default: ben-ge-800 directory\n",
    "        split    -- string, describes the split to be instantiated, either `train`, `val` or `test`\n",
    "        s2_bands -- list of Sentinel-2 bands to be extracted, default: all bands\n",
    "\n",
    "        returns:\n",
    "        BENGE object\n",
    "        \"\"\"\n",
    "        super(BENGE, self).__init__()\n",
    "\n",
    "        # store some definitions\n",
    "        self.s2_bands = s2_bands\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # read in relevant data files and definitions\n",
    "        self.name = self.data_dir.split(\"/\")[-1]\n",
    "        self.split = split\n",
    "        self.meta = pd.read_csv(f\"{self.data_dir}/{self.name}_meta.csv\")\n",
    "\n",
    "        # extract prevalent lulc label for each sample\n",
    "        ewc = pd.read_csv(f\"{self.data_dir}/{self.name}_esaworldcover.csv\")\n",
    "        self.meta.loc[:, 'lulc'] = np.argmax(ewc.loc[:, 'tree_cover':'moss_and_lichen'].values, axis=1)\n",
    "              \n",
    "        # we shuffle the indices in the meta file and then select the first 500 samples for training, 150 for validation and 150 for testing\n",
    "        if split == 'train':\n",
    "            self.meta = self.meta.iloc[0:500]\n",
    "        if split == 'val':\n",
    "            self.meta = self.meta.iloc[500:650]\n",
    "        if split == 'test':\n",
    "            self.meta = self.meta.iloc[650:800]\n",
    "        \n",
    "        #self.meta = self.meta.loc[self.meta.split == split, :]  # filter by split\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return sample `idx` as dictionary from the dataset.\"\"\"\n",
    "        sample_info = self.meta.iloc[idx]\n",
    "        patch_id = sample_info.patch_id  # extract Sentinel-2 patch id\n",
    "\n",
    "        # retrieve Sentinel-2 data\n",
    "        s2 = np.empty((4, 120, 120))\n",
    "        for i, band in enumerate(self.s2_bands):\n",
    "            with rio.open(f\"{self.data_dir}/sentinel-2/{patch_id}/{patch_id}_B0{band}.tif\") as dataset:\n",
    "                data = dataset.read(1)\n",
    "            s2[i,:,:] = data\n",
    "        s2 = np.clip(s2.astype(float) / 10000, 0, 1)  # normalize Sentinel-2 data\n",
    "\n",
    "        # create sample dictionary containing all the data\n",
    "        sample = {\n",
    "            \"patch_id\": patch_id,  # Sentinel-2 id of this patch\n",
    "            \"s2\": torch.from_numpy(s2).float(),  # Sentine;-2 data [4, 120, 120]\n",
    "            \"lulc\": torch.tensor(sample_info.lulc).long(),  # most prevalent ESA WorldCover lulc class\n",
    "            }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return length of this dataset.\"\"\"\n",
    "        return self.meta.shape[0]\n",
    "\n",
    "    def display(self, idx):\n",
    "        \"\"\"Method to display a data sample, consisting of the Sentinel-2 image.\n",
    "        \n",
    "        positional arguments:\n",
    "        idx -- sample index\n",
    "        \"\"\"\n",
    "\n",
    "        # retrieve sample\n",
    "        sample = self[idx]\n",
    "\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "        \n",
    "        # display Sentinel-2 image\n",
    "        img_rgb = np.dstack(sample['s2'][0:3].numpy()[::-1])  # extract RGB, reorder, and perform a deep stack (shape: 120, 120, 3)\n",
    "        ax.imshow((img_rgb-np.min(img_rgb))/(np.max(img_rgb)-np.min(img_rgb)))\n",
    "        ax.set_title(ewc_label_names[sample['lulc'].numpy()])\n",
    "        ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XShJZLzc5anM"
   },
   "source": [
    "We can now instantiate the different splits for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PygqMbD05anN"
   },
   "outputs": [],
   "source": [
    "train_data = BENGE(split='train')\n",
    "val_data = BENGE(split='val')\n",
    "test_data = BENGE(split='test')\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmzCS6cP5anN"
   },
   "source": [
    "We can retrieve a single sample simply by indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6iJ4PFY5anN"
   },
   "outputs": [],
   "source": [
    "train_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.display(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCfRser65anN"
   },
   "source": [
    "For Neural Network training we have to define data loaders. When we do so, we have to define the batch size, which is typically limited by the GPU RAM during training. For evaluation purposes, we can typically pick a larger batch size, since we need less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vigNcosR5anO"
   },
   "outputs": [],
   "source": [
    "train_batchsize = 8\n",
    "eval_batchsize = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=train_batchsize, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8_bVEfRpM4W"
   },
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOK06D2caPjB"
   },
   "source": [
    "We build a very simple Convolutional Neural Network that consists of three convolutional layers and two linear layers to learn our image classification task. \n",
    "\n",
    "The **convolutional layers** will be set up as follows:\n",
    "* The first convolutional layer will take in our input images (120 x 120 pixels) with 4 channels. Aiming for 8 output channels and using a kernel size of 5 and a stride of 2 (to decrease the size of the resulting feature maps), this will result in a feature map of size `[8, 58, 58]`. This layer will be followed by ReLU for non-linear activations and an additional Maxpooling layer that reduces the size of the output feature map by a factor of 2, resulting in an output of size `[8, 29, 29]`.\n",
    "* The second convolutional layer will result in 16 output channels and also use a kernel size of 5 and a stride of 2. Followed by ReLU and Maxpooling, the output feature map will be of size `[16, 6, 6]`.\n",
    "\n",
    "At the intersection between the convolutional layers and the linear layers we have to transform the feature maps of size `[16, 6, 6]` into a vector. This vector will have a lenght of $16 \\times 6 \\times 6 = 576$. \n",
    "\n",
    "This vector will serve as input for our **linear layers**:\n",
    "* The first linear layer will take an input of length 576 and output a vector of length 100. This layer will be followed by ReLU.\n",
    "* The second linear layer will take the vector of length 100 and result in a vector of length 11, which represents the number of land-use/land-cover labels in our dataset. This layer will not be followed by ReLU, since we don't want to use non-linear activations here. Instead, we will apply a logsoftmax function to output values that we can interpret as classification probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaJLON_ckFW_"
   },
   "outputs": [],
   "source": [
    "class BENGENet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BENGENet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 8, 5, stride=2) # 4 channels to 8 channels with a kernel size of 5\n",
    "        self.conv2 = nn.Conv2d(8, 16, 5, stride=2) # 4 channels to 8 channels with a kernel size of 5\n",
    "        \n",
    "        self.linear1 = nn.Linear(16*6*6, 100, bias=True)\n",
    "        self.linear2 = nn.Linear(100, 11, bias=True)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(2)  # maxpooling by a factor of 2\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # convolutional layers\n",
    "        x = self.maxpool(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "\n",
    "        # reshape feature maps for linear layers\n",
    "        x = x.view(-1, 16*6*6)\n",
    "\n",
    "        # linear layers\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.logsoftmax(self.linear2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the model and we're ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xm1HYSr5anO"
   },
   "outputs": [],
   "source": [
    "model = BENGENet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy_LH3aUpM4Z"
   },
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F66Xr5Xy5anP"
   },
   "source": [
    "First of all, let's verify if a GPU is available on our compute machine. If not, the CPU will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xb_5oxnbcId"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device used: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl4509B-8kIZ"
   },
   "source": [
    "Before we can implement the training pipeline we have to define two more things: a Loss function and an optimizer that will update our model weights during training. We also define our evaluation metric, for which we use the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJvU_eEr5anP"
   },
   "outputs": [],
   "source": [
    "# we will use the cross entropy loss\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "# we will use the Adam optimizer\n",
    "learning_rate = 0.0001\n",
    "opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# we instantiate the accuracy metric\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mxynwEl5anP"
   },
   "source": [
    "Now, we have to move the model and the loss function on the GPU, since the computationally heavy work will be conducted there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAXnEvAu5anP"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "loss.to(device)\n",
    "accuracy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEbPAZwn5anQ"
   },
   "source": [
    "Finally, we can implement our training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn8Kiqd-iR3l"
   },
   "outputs": [],
   "source": [
    "epochs = 30  # training for 10 epochs\n",
    "\n",
    "train_losses_epochs = []\n",
    "val_losses_epochs = []\n",
    "train_accs_epochs = []\n",
    "val_accs_epochs = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    # we perform training for one epoch\n",
    "    model.train()   # it is very important to put your model into training mode!\n",
    "    for samples in tqdm(train_dataloader):\n",
    "        # we extract the input data (Sentinel-2)\n",
    "        x = samples['s2'].to(device)\n",
    "\n",
    "        # now we extract the target (lulc class) and move it to the gpu\n",
    "        y = samples['lulc'].to(device)\n",
    "\n",
    "        # we make a prediction with our model\n",
    "        output = model(x)\n",
    "\n",
    "        # we reset the graph gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # we determine the classification loss\n",
    "        loss_train = loss(output, y)\n",
    "\n",
    "        # we run a backward pass to comput the gradients\n",
    "        loss_train.backward()\n",
    "\n",
    "        # we update the network paramaters\n",
    "        opt.step()\n",
    "\n",
    "        # we write the mini-batch loss and accuracy into the corresponding lists\n",
    "        train_losses.append(loss_train.detach().cpu())\n",
    "        train_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
    "\n",
    "    # we evaluate the current state of the model on the validation dataset\n",
    "    model.eval()   # it is very important to put your model into evaluation mode!\n",
    "    with torch.no_grad():\n",
    "        for samples in tqdm(val_dataloader):\n",
    "            # we extract the input data (Sentinel-2)\n",
    "            x = samples['s2'].to(device)\n",
    "\n",
    "            # now we extract the target (lulc class) and move it to the gpu\n",
    "            y = samples['lulc'].to(device)\n",
    "\n",
    "            # we make a prediction with our model\n",
    "            output = model(x)\n",
    "\n",
    "            # we determine the classification loss\n",
    "            loss_val = loss(output, y)\n",
    "\n",
    "            # we write the mini-batch loss and accuracy into the corresponding lists\n",
    "            val_losses.append(loss_val.detach().cpu())\n",
    "            val_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
    "\n",
    "    train_losses_epochs.append(np.mean(train_losses))\n",
    "    train_accs_epochs.append(np.mean(train_accs))\n",
    "    val_losses_epochs.append(np.mean(val_losses))\n",
    "    val_accs_epochs.append(np.mean(val_accs))\n",
    "\n",
    "    print(\"epoch {}: train: loss={}, acc={}; val: loss={}, acc={}\".format(\n",
    "        ep, train_losses_epochs[-1], train_accs_epochs[-1], \n",
    "        val_losses_epochs[-1], val_accs_epochs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S81HUFy5anQ"
   },
   "source": [
    "Training progress looks good: train and validation losses are decreasing, accuracies are increasing.\n",
    "\n",
    "Let's plot the available metrics as a function of the number of training iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hApnOKv5anQ"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
    "\n",
    "ax[0].plot(np.arange(1, len(train_losses_epochs)+1), train_losses_epochs, label='Train', color='blue')\n",
    "ax[0].plot(np.arange(1, len(val_losses_epochs)+1), val_losses_epochs, label='Val', color='red')\n",
    "ax[0].set_xlabel('Iterations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(1, len(train_accs_epochs)+1), train_accs_epochs, label='Train', color='blue')\n",
    "ax[1].plot(np.arange(1, len(val_accs_epochs)+1), val_accs_epochs, label='Val', color='red')\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2x-UByx5anQ"
   },
   "source": [
    "The model learns well and we stopped the learning process before overfitting sets in. Let's evaluate the model again on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k18Zn4Ff5anR"
   },
   "outputs": [],
   "source": [
    "test_accs = []\n",
    "predictions = []\n",
    "groundtruths = []\n",
    "\n",
    "model.eval()   # it is very important to put your model into evaluation mode!\n",
    "with torch.no_grad():\n",
    "    for samples in tqdm(test_dataloader):\n",
    "        x = samples['s2'].to(device)\n",
    "\n",
    "        # now we extract the target (lulc class) and move it to the gpu\n",
    "        y = samples['lulc'].to(device)\n",
    "        groundtruths.append(y.cpu())\n",
    "\n",
    "        # we make a prediction with our model\n",
    "        output = model(x)\n",
    "\n",
    "        predictions.append(np.argmax(output.cpu().numpy(), axis=1))\n",
    "\n",
    "        # we determine the classification loss\n",
    "        loss_val = loss(output, y)\n",
    "\n",
    "        # we write the mini-nbatch loss and accuracy into the corresponding lists\n",
    "        test_accs.append(accuracy(torch.argmax(output, dim=1), y).cpu().numpy())\n",
    "\n",
    "print('test dataset accuracy:', np.mean(test_accs))\n",
    "\n",
    "# flatten predictions and groundtruths\n",
    "predictions = np.concatenate(predictions).ravel()\n",
    "groundtruths = np.concatenate(groundtruths).ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset performance is very close to the validation dataset performance, which is a good sign.\n",
    "\n",
    "Finally, let's have a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "\n",
    "# plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    groundtruths, predictions,\n",
    "    display_labels=[ewc_label_names[i] for i in np.unique(groundtruths)],\n",
    "    normalize='true',\n",
    "    ax=ax)\n",
    "\n",
    "# rotate x labels for better readability\n",
    "ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is far from performing perfectly: a significant number of classes is misclassified as tree cover or grassland. This confusion is to be expected, since the problem is ill-posed: naturally, most images will include more than a single class. Therefore, only the largest classes will be predicted by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Modify the design of the CNN. Will the results improve if you add more convolutional/linear layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "6e741a586f6b68b554925f4bea211b6852c45fbf3b1f1159871bb4b38b6bf4de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
