{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Classification with Deep Learning for Sentinel-2 Satellite Imagery\n",
        "\n",
        "Michael Mommert, Stuttgart University of Applied Sciences, 2024\n",
        "\n",
        "This Notebook introduces the workflow for supervised learning with Neural Networks. We will apply these methods to Sentinel-2 satellite images from the [*ben-ge-800* dataset](https://zenodo.org/records/12941231) to perform pixel-wise classification based on land-use/land-cover data from the [ESAWorldCover dataset](https://esa-worldcover.org/en). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy \\\n",
        "    scipy \\\n",
        "    pandas \\\n",
        "    matplotlib \\\n",
        "    rasterio \\\n",
        "    scikit-learn \\\n",
        "    torch \\\n",
        "    torchmetrics \\\n",
        "    tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Content\n",
        "\n",
        "1. [Setting up the environment and data download](#setup)\n",
        "2. [Data Inspection](#data_inspection)\n",
        "3. [Data Handling](#data)\n",
        "4. [Model Implementation](#model)\n",
        "5. [Training and Validation Pipeline](#train-val)\n",
        "6. [Hyperparameter Tuning](#hyperpars)\n",
        "7. [Evaluation](#evaluation)\n",
        "8. [Inference](#inference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='setup'></a>\n",
        "## 1. Setup and Data Download\n",
        "\n",
        "We're setting up our Python environment for this tutorial by installing and importing the necessary modules and packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi5NrxiUYT_m"
      },
      "outputs": [],
      "source": [
        "# system level modules for handling files and file structures\n",
        "import os\n",
        "import tarfile\n",
        "import copy\n",
        "\n",
        "# scipy ecosystem imports for numerics, data handling and plotting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# pytorch and helper modules\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# utils\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# rasterio for reading in satellite image data\n",
        "import rasterio as rio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We download the *ben-ge-800* dataset and unpack it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://zenodo.org/records/12941231/files/ben-ge-800.tar.gz?download=1 -O ben-ge-800.tar.gz\n",
        "  \n",
        "tarfile = tarfile.open('ben-ge-800.tar.gz')  # open ben-ge-800 tarball \n",
        "tarfile.extractall('./')  # extract tarball\n",
        "\n",
        "data_base_path = os.path.join(os.path.abspath('.'), 'ben-ge-800')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvTJ9WPQOxon"
      },
      "source": [
        "**ben-ge-800** contains samples for 800 locations with co-located Sentinel-1 SAR data, Sentinel-2 multispectral data, elevation data, land-use/land-cover data, as well as environmental data. **ben-ge-800** is a subset of the much larger **ben-ge** dataset (see [https://github.com/HSG-AIML/ben-ge](https://github.com/HSG-AIML/ben-ge) for details.) We deliberately use a very small subset of **ben-ge** to enable reasonable runtimes for the examples shown in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yver4nem5anJ"
      },
      "source": [
        "The environment is now set up and the data in place. Before we define the dataset classes and dataloaders to access the data efficiently, we fix some random seeds to obtain reproduceable results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOZcKSCT5anJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)     # sets the seed value in Numpy\n",
        "torch.manual_seed(42)  # sets the seed value in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4UnJK1N5anJ"
      },
      "source": [
        "<a id='data_inspection'></a>\n",
        "## 2. Data Inspection\n",
        "\n",
        "Before we start implementing our model, let's have a look at the data. In this notebook, we need two different data products that are available for every single sample in the dataset:\n",
        "* Sentinel-2 multispectral data: 12-band Level-2A images of size 120x120; we will restrict ourselves to the 4 bands that carry 10m-resolution imaging data (bands 2, 3, 4 and 8)\n",
        "* [ESAWorldCover](https://esa-worldcover.org/en) land-use/land-cover labels: for each pixel of each image, a class label is provided that corresponds to one of 11 different classes.\n",
        "\n",
        "Let's have a look at how to access the different data products:\n",
        "\n",
        "### Sentinel-2\n",
        "\n",
        "Sentinel-2 data are located in the `ben-ge-800/sentinel-2/` directory. Each sample has its own subdirectory; random sample is named `S2B_MSIL2A_20170814T100029_90_11`. For each sample, 12 `.tif` files are available, one for each band. Let's open the red band data for this sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bUfVR8w5anK"
      },
      "outputs": [],
      "source": [
        "dataset = rio.open(\"ben-ge-800/sentinel-2/S2B_MSIL2A_20170814T100029_90_11/S2B_MSIL2A_20170814T100029_90_11_B04.tif\")  # open tif file with rasterio\n",
        "data = dataset.read(1)  # read data (band 1, since there is only one band)\n",
        "data.shape, data.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix76S6xr5anK"
      },
      "source": [
        "The data has indeed the shape 120x120 pixels (per band) and the data is stored as 16-bit integer values. Let's plot the data as a greyscale image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvXtYyHM5anK"
      },
      "outputs": [],
      "source": [
        "plt.imshow((data-np.min(data))/(np.max(data)-np.min(data)), cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtnpvFdC5anK"
      },
      "source": [
        "To plot a color image, we have to also open and read the blue and green bands. This is straightforward, since R, G, and B have the same spatial resolutions. Most other bands, however, have different spatial resolutions. Therefore, we have to resample the data to 10m resolution. Let's do this for all bands and then plot the true color information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lewvc8RC5anK"
      },
      "outputs": [],
      "source": [
        "# an ordered list of all the bands to be extracted\n",
        "s2_bands = [2, 3, 4, 8]  # B, G, R, NIR\n",
        "\n",
        "# read all bands for one sample\n",
        "img = np.empty((4, 120, 120))\n",
        "for i, band in enumerate(s2_bands):\n",
        "\n",
        "    # read corresponding data file and upsample based on resampling factor\n",
        "    with rio.open(f\"ben-ge-800/sentinel-2/S2B_MSIL2A_20170814T100029_90_11/S2B_MSIL2A_20170814T100029_90_11_B0{band}.tif\") as dataset:\n",
        "                data = dataset.read()\n",
        "    img[i,:,:] = data\n",
        "\n",
        "# plot the RGB information for that sample\n",
        "img_rgb = np.dstack(img[0:3][::-1])  # extract RGB, reorder, and perform a deep stack (shape: 120, 120, 3)\n",
        "plt.imshow((img_rgb-np.min(img_rgb))/(np.max(img_rgb)-np.min(img_rgb)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNgn5yV5anK"
      },
      "source": [
        "In order to normalize our Sentinel-2 data, we simply divide the pixel values in each band by 10000 and clip the range from 0 to 1. This provides a reasonable value in each band:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyCy5gMP5anK"
      },
      "outputs": [],
      "source": [
        "np.average(np.clip(img/10000, 0, 1), axis=(1,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr4YIMdq5anL"
      },
      "source": [
        "### Land-use/land-cover data\n",
        "\n",
        "Finally, we will read in the ESAWorldCover land-use/land-cover data. Since the lulc data form a map with the same size as the image, the data is stored as a TIF file, just like the image data. Let's open the lulc data file that corresponds to the scene shown above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd7_ZDA45anL"
      },
      "outputs": [],
      "source": [
        "with rio.open(\"ben-ge-800/esaworldcover/S2B_MSIL2A_20170814T100029_90_11_esaworldcover.tif\") as d:\n",
        "    ewc_data = d.read(1)\n",
        "\n",
        "ewc_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class labels are encoded in numerical codes. Before we continue, we define a colormap for displaying the lulc data in a visually appealing way and we define the class label names for better understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define ESA WorldCover colormap\n",
        "COLOR_CATEGORIES = [\n",
        "    (0, 100, 0),\n",
        "    (255, 187, 34),\n",
        "    (255, 255, 76),\n",
        "    (240, 150, 255),\n",
        "    (250, 0, 0),\n",
        "    (180, 180, 180),\n",
        "    (240, 240, 240),\n",
        "    (0, 100, 200),\n",
        "    (0, 150, 160),\n",
        "    (0, 207, 117),\n",
        "    (250, 230, 160)]\n",
        "cmap_all = mpl.colors.ListedColormap(np.array(COLOR_CATEGORIES)/255.)\n",
        "\n",
        "# class label names\n",
        "ewc_label_names = [\"tree_cover\", \"shrubland\", \"grassland\", \"cropland\", \"built-up\",\n",
        "                   \"bare/sparse_vegetation\", \"snow_and_ice\",\"permanent_water_bodies\",\n",
        "                   \"herbaceous_wetland\", \"mangroves\",\"moss_and_lichen\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we turn the numerical codes into a continuous range of integer values; the order corresponds to the order used in the color map and the class label name list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ewc_mask = ewc_data.astype(float)   \n",
        "ewc_mask[ewc_mask == 100] = 110  # fix some irregular class labels\n",
        "ewc_mask[ewc_mask == 95] = 100   # fix some irregular class labels\n",
        "ewc_mask = ewc_mask / 10 - 1 # transform to scale [0, 11]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot the lulc map next to the satellite image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8,4))\n",
        "\n",
        "ax[0].imshow((img_rgb-np.min(img_rgb))/(np.max(img_rgb)-np.min(img_rgb)))\n",
        "ax[1].imshow(ewc_mask, cmap=cmap_all, vmin=0, vmax=11, interpolation='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-le0XROa95M"
      },
      "source": [
        "<a id='data'></a>\n",
        "## 3. Data Handling\n",
        "\n",
        "In the following, we implement a dataset class that combines the data access methods introduced above for the two data modalities. The dataset class provides easy and homogeneous access to the data on a per-sample basis. As part of the dataset class, we apply data normalizations and output all numeric features as *Pytorch* tensors; tensors are the Pytorch equivalent of Numpy arrays but tensors can use GPU infrastructure for more efficient computations. Finally, we use predefined dataset splits: we can generate a training, validation and test dataset.\n",
        "\n",
        "Also note that, unlike semantic segmentation, we will set this problem us as a classification problem, which means that each pixel will be treated as a separate sample, ignoring its neighborhood. While this approach is inferior to a semantic segmentation approach, the Neural Network architecture to tackle this problem is much simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdVdNb2qP7OQ"
      },
      "outputs": [],
      "source": [
        "class BENGE(Dataset):\n",
        "    \"\"\"A dataset class implementing the Sentinel-1, Sentinel-2 and ESAWorldCover data modalities.\"\"\"\n",
        "    def __init__(self, \n",
        "                 data_dir=data_base_path, \n",
        "                 split='train',\n",
        "                 s2_bands=[2, 3, 4, 8]):\n",
        "        \"\"\"Dataset class constructor\n",
        "\n",
        "        keyword arguments:\n",
        "        data_dir -- string containing the path to the base directory of ben-ge dataset, default: ben-ge-800 directory\n",
        "        split    -- string, describes the split to be instantiated, either `train`, `val` or `test`\n",
        "        s2_bands -- list of Sentinel-2 bands to be extracted, default: all bands\n",
        "\n",
        "        returns:\n",
        "        BENGE object\n",
        "        \"\"\"\n",
        "        super(BENGE, self).__init__()\n",
        "\n",
        "        # store some definitions\n",
        "        self.s2_bands = s2_bands\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # read in relevant data files and definitions\n",
        "        self.name = self.data_dir.split(\"/\")[-1]\n",
        "        self.split = split\n",
        "        self.meta = pd.read_csv(f\"{self.data_dir}/{self.name}_meta.csv\")\n",
        "\n",
        "        # we shuffle the indices in the meta file and then select the first 500 samples for training, 150 for validation and 150 for testing\n",
        "        self.meta.sample(frac=1).reset_index(drop=True)\n",
        "        if split == 'train':\n",
        "            self.meta = self.meta.iloc[0:500]\n",
        "        if split == 'val':\n",
        "            self.meta = self.meta.iloc[500:650]\n",
        "        if split == 'test':\n",
        "            self.meta = self.meta.iloc[650:800]\n",
        "        \n",
        "        #self.meta = self.meta.loc[self.meta.split == split, :]  # filter by split\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return sample `idx` as dictionary from the dataset.\"\"\"\n",
        "        sample_info = self.meta.iloc[idx]\n",
        "        patch_id = sample_info.patch_id  # extract Sentinel-2 patch id\n",
        "\n",
        "        # retrieve Sentinel-2 data\n",
        "        s2 = np.empty((4, 120, 120))\n",
        "        for i, band in enumerate(self.s2_bands):\n",
        "            with rio.open(f\"{self.data_dir}/sentinel-2/{patch_id}/{patch_id}_B0{band}.tif\") as dataset:\n",
        "                data = dataset.read(1)\n",
        "            s2[i,:,:] = data\n",
        "        s2 = np.clip(s2.astype(float) / 10000, 0, 1)  # normalize Sentinel-2 data\n",
        "\n",
        "        # extract lulc data\n",
        "        with rio.open(f\"{self.data_dir}/esaworldcover/{patch_id}_esaworldcover.tif\") as dataset:\n",
        "            ewc_data = dataset.read(1)\n",
        "        ewc_mask = ewc_data.astype(float)   \n",
        "        ewc_mask[ewc_mask == 100] = 110  # fix some irregular class labels\n",
        "        ewc_mask[ewc_mask == 95] = 100   # fix some irregular class labels\n",
        "        ewc_mask = ewc_mask / 10 - 1 # transform to scale [0, 11]\n",
        "\n",
        "        # create sample dictionary containing all the data\n",
        "        sample = {\n",
        "            \"patch_id\": patch_id,  # Sentinel-2 id of this patch\n",
        "            \"s2\": torch.from_numpy(s2).float(),  # Sentine;-2 data [4, 120, 120]\n",
        "            \"lulc\": torch.from_numpy(ewc_mask).long(),  # ESA WorldCover lulc classes per pixel [120, 120]\n",
        "            }\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return length of this dataset.\"\"\"\n",
        "        return self.meta.shape[0]\n",
        "\n",
        "    def display(self, idx, pred=None):\n",
        "        \"\"\"Method to display a data sample, consisting of the Sentinel-2 image and lulc map, and potentially a corresponding prediction.\n",
        "        \n",
        "        positional arguments:\n",
        "        idx -- sample index\n",
        "        \n",
        "        keyword arguments:\n",
        "        pred -- prediction tensor\n",
        "        \"\"\"\n",
        "\n",
        "        # retrieve sample\n",
        "        sample = self[idx]\n",
        "\n",
        "        if pred is None:\n",
        "            f, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 4))\n",
        "        else:\n",
        "            f, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(12, 4))\n",
        "\n",
        "        # display Sentinel-2 image\n",
        "        img_rgb = np.dstack(sample['s2'][0:3].numpy()[::-1])  # extract RGB, reorder, and perform a deep stack (shape: 120, 120, 3)\n",
        "        ax[0].imshow((img_rgb-np.min(img_rgb))/(np.max(img_rgb)-np.min(img_rgb)))\n",
        "        ax[0].set_title('Sentinel-2')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "\n",
        "        # display lulc map\n",
        "        ax[1].imshow(sample['lulc'], cmap=cmap_all, vmin=0, vmax=11, interpolation='nearest')\n",
        "        ax[1].set_title('LULC')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # display prediction, if available\n",
        "        if pred is not None:\n",
        "            ax[2].imshow(pred, cmap=cmap_all, vmin=0, vmax=11, interpolation='nearest')\n",
        "            ax[2].set_title('Prediction')\n",
        "            ax[2].axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShJZLzc5anM"
      },
      "source": [
        "We can now instantiate the different splits for this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PygqMbD05anN"
      },
      "outputs": [],
      "source": [
        "train_data = BENGE(split='train')\n",
        "val_data = BENGE(split='val')\n",
        "test_data = BENGE(split='test')\n",
        "\n",
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzCS6cP5anN"
      },
      "source": [
        "We can retrieve a single sample simply by indexing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6iJ4PFY5anN"
      },
      "outputs": [],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's display this sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.display(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfRser65anN"
      },
      "source": [
        "For Neural Network training we have to define data loaders. When we do so, we have to define the batch size, which is typically limited by the GPU RAM during training. For evaluation purposes, we can typically pick a larger batch size, since we need less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigNcosR5anO"
      },
      "outputs": [],
      "source": [
        "train_batchsize = 8\n",
        "eval_batchsize = 16\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=train_batchsize, num_workers=4, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=eval_batchsize, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8_bVEfRpM4W"
      },
      "source": [
        "<a id='model'></a>\n",
        "## 4. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOK06D2caPjB"
      },
      "source": [
        "In this notebook, we build a very simple 3-layer MLP to learn the task of classification. \n",
        "\n",
        "The model architecture will look as follows:\n",
        "* As **input layer** we use a linear layer with 4 inputs (4 Sentinel-2 bands) and 50 outputs,\n",
        "* the **hidden layer** has 50 inputs and 30 outputs, and\n",
        "* the **output layer** has 30 inputs and 11 outputs (11 different lulc classes).\n",
        "\n",
        "The input and hidden layers will use ReLU activation function and the output layer will be followed by a softmax function.\n",
        "\n",
        "The network architecture is deliberately kept very small to enable fast training (even if you don't have a GPU at hand). Naturally, larger networks would result in better predictions.\n",
        "\n",
        "We build the network architecture using PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaJLON_ckFW_"
      },
      "outputs": [],
      "source": [
        "class BENGENet(nn.Module):\n",
        "    \n",
        "    # class constructor: here we define all the \"building blocks\" of our network\n",
        "    def __init__(self):\n",
        "        super(BENGENet, self).__init__()\n",
        "        \n",
        "        # input layer: layer 1 - in 4, out 50\n",
        "        self.linear1 = nn.Linear(4, 50, bias=True)\n",
        "        \n",
        "        # hidden layer: layer 2 - in 50, out 30\n",
        "        self.linear2 = nn.Linear(50, 30, bias=True)\n",
        "        \n",
        "        # output layer: layer 3 - in 30, out 11\n",
        "        self.linear3 = nn.Linear(30, 11)\n",
        "\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "\n",
        "        # reshape Sentinel-2 image to list of pixels\n",
        "        x = torch.movedim(x.float(), 1, 3).reshape(-1, 4)\n",
        "\n",
        "        # we send the reshaped input through the invidiual layers \n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.logsoftmax(self.linear3(x))\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we instantiate the model and we're ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xm1HYSr5anO"
      },
      "outputs": [],
      "source": [
        "model = BENGENet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_LH3aUpM4Z"
      },
      "source": [
        "<a id='train-val'></a>\n",
        "## 5. Training and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F66Xr5Xy5anP"
      },
      "source": [
        "First of all, let's verify if a GPU is available on our compute machine. If not, the CPU will be used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xb_5oxnbcId"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Device used: {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl4509B-8kIZ"
      },
      "source": [
        "Before we can implement the training pipeline we have to define two more things: a Loss function and an optimizer that will update our model weights during training. We also define our evaluation metric, for which we use the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJvU_eEr5anP"
      },
      "outputs": [],
      "source": [
        "# we will use the cross entropy loss\n",
        "loss = nn.NLLLoss()\n",
        "\n",
        "# we will use the Adam optimizer\n",
        "learning_rate = 0.001\n",
        "opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# we instantiate the accuracy metric\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxynwEl5anP"
      },
      "source": [
        "Now, we have to move the model and the loss function on the GPU, since the computationally heavy work will be conducted there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAXnEvAu5anP"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "loss.to(device)\n",
        "accuracy.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbPAZwn5anQ"
      },
      "source": [
        "Finally, we can implement our training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn8Kiqd-iR3l"
      },
      "outputs": [],
      "source": [
        "epochs = 30  # training for 30 epochs\n",
        "\n",
        "train_losses_epochs = []\n",
        "val_losses_epochs = []\n",
        "train_accs_epochs = []\n",
        "val_accs_epochs = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # we perform training for one epoch\n",
        "    model.train()   # it is very important to put your model into training mode!\n",
        "    for samples in tqdm(train_dataloader):\n",
        "        # we extract the input data (Sentinel-2)\n",
        "        x = samples['s2'].to(device)\n",
        "\n",
        "        # now we extract the target (lulc class) and move it to the gpu\n",
        "        y = samples['lulc'].view(-1).to(device)\n",
        "\n",
        "        # we make a prediction with our model\n",
        "        output = model(x)\n",
        "\n",
        "        # we reset the graph gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # we determine the classification loss\n",
        "        loss_train = loss(output, y)\n",
        "\n",
        "        # we run a backward pass to comput the gradients\n",
        "        loss_train.backward()\n",
        "\n",
        "        # we update the network paramaters\n",
        "        opt.step()\n",
        "\n",
        "        # we write the mini-batch loss and accuracy into the corresponding lists\n",
        "        train_losses.append(loss_train.detach().cpu())\n",
        "        train_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
        "\n",
        "    #print(train_accs)\n",
        "    #print(output)\n",
        "\n",
        "    # we evaluate the current state of the model on the validation dataset\n",
        "    model.eval()   # it is very important to put your model into evaluation mode!\n",
        "    with torch.no_grad():\n",
        "        for samples in tqdm(val_dataloader):\n",
        "            # we extract the input data (Sentinel-2)\n",
        "            x = samples['s2'].to(device)\n",
        "\n",
        "            # now we extract the target (lulc class) and move it to the gpu\n",
        "            y = samples['lulc'].view(-1).to(device)\n",
        "\n",
        "            # we make a prediction with our model\n",
        "            output = model(x)\n",
        "\n",
        "            # we determine the classification loss\n",
        "            loss_val = loss(output, y)\n",
        "\n",
        "            # we write the mini-batch loss and accuracy into the corresponding lists\n",
        "            val_losses.append(loss_val.detach().cpu())\n",
        "            val_accs.append(accuracy(torch.argmax(output, dim=1), y).detach().cpu())\n",
        "\n",
        "    train_losses_epochs.append(np.mean(train_losses))\n",
        "    train_accs_epochs.append(np.mean(train_accs))\n",
        "    val_losses_epochs.append(np.mean(val_losses))\n",
        "    val_accs_epochs.append(np.mean(val_accs))\n",
        "\n",
        "    print(\"epoch {}: train: loss={}, acc={}; val: loss={}, acc={}\".format(\n",
        "        ep, train_losses_epochs[-1], train_accs_epochs[-1], \n",
        "        val_losses_epochs[-1], val_accs_epochs[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S81HUFy5anQ"
      },
      "source": [
        "Training progress looks good: train and validation losses are decreasing, accuracies are increasing.\n",
        "\n",
        "Let's plot the available metrics as a function of the number of training iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hApnOKv5anQ"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
        "\n",
        "ax[0].plot(np.arange(1, len(train_losses_epochs)+1), train_losses_epochs, label='Train', color='blue')\n",
        "ax[0].plot(np.arange(1, len(val_losses_epochs)+1), val_losses_epochs, label='Val', color='red')\n",
        "ax[0].set_xlabel('Iterations')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(np.arange(1, len(train_accs_epochs)+1), train_accs_epochs, label='Train', color='blue')\n",
        "ax[1].plot(np.arange(1, len(val_accs_epochs)+1), val_accs_epochs, label='Val', color='red')\n",
        "ax[1].set_xlabel('Iterations')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2x-UByx5anQ"
      },
      "source": [
        "We see how the losses drop and the accuracy increases for both the training and validation dataset. Since the validation metrics follow the training metrics, we do not see any obvious signs for overfitting. Based on the validation dataset, we reach an accuracy of 47% for our multi-class classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyfLhN0O5anR"
      },
      "source": [
        "<a id='hyperpars'></a>\n",
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "We trained the model successfully. But did we manage to get the best possible result?\n",
        "\n",
        "In order to find out, we have to perform hyperparameter tuning. The only obvious hyperparameter of our model is the learning rate. Architectural considerations can also be considered as hyperparameters, but we will keep our architecture fixed.\n",
        "\n",
        "As a result, we only have to tune the learning rate. We could try other learning rate values, such as 0.01 or 0.0001, or use a scheduler that modifies the learning rate as a function of the training progress.\n",
        "\n",
        "However, we will skip this tuning process here to save some time. Instead, we will use the trained model \"as is\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsjgpC5d5anR"
      },
      "source": [
        "<a id='evaluation'></a>\n",
        "## 7. Evaluation\n",
        "\n",
        "After finishing the hyperparameter tuning, we can perform the final evaluation of our model. We will again use the accuracy metric, but could easily replace it or add additional metrics.\n",
        "\n",
        "To properly evaluate our model we must use the test dataset in the evaluation process. Since the model has never seen the test dataset before, it will provide a realistic estimate of the performance of the model on previously unseen data.\n",
        "\n",
        "The evaluation uses more or less the same code that we used to evaluate our model during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k18Zn4Ff5anR"
      },
      "outputs": [],
      "source": [
        "test_accs = []\n",
        "predictions = []\n",
        "groundtruths = []\n",
        "\n",
        "model.eval()   # it is very important to put your model into evaluation mode!\n",
        "with torch.no_grad():\n",
        "    for samples in tqdm(test_dataloader):\n",
        "        x = samples['s2'].to(device)\n",
        "\n",
        "        # now we extract the target (lulc class) and move it to the gpu\n",
        "        y = samples['lulc'].view(-1).to(device)\n",
        "        groundtruths.append(y.cpu())\n",
        "\n",
        "        # we make a prediction with our model\n",
        "        output = model(x)\n",
        "\n",
        "        predictions.append(np.argmax(output.cpu().numpy(), axis=1))\n",
        "\n",
        "        # we determine the classification loss\n",
        "        loss_val = loss(output, y)\n",
        "\n",
        "        # we write the mini-nbatch loss and accuracy into the corresponding lists\n",
        "        test_accs.append(accuracy(torch.argmax(output, dim=1), y).cpu().numpy())\n",
        "\n",
        "print('test dataset accuracy:', np.mean(test_accs))\n",
        "\n",
        "# flatten predictions and groundtruths\n",
        "predictions = np.concatenate(predictions).ravel()\n",
        "groundtruths = np.concatenate(groundtruths).ravel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhOjslEb5anR"
      },
      "source": [
        "The test dataset accuracy is 50%, which is even higher than the validation accuracy.\n",
        "\n",
        "**Question**: Don't get fooled by the accuracy. The model performs not too badly. What is the baseline accuracy you would expect for for random guessing in a multi-class classification problem, if there are 11 different classes?\n",
        "\n",
        "Now, let's also have a look at the performance on a per-class basis via the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqHR2Vj35anR"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 1, figsize=(8,6))\n",
        "\n",
        "# plot the confusion matrix\n",
        "disp = ConfusionMatrixDisplay.from_predictions(\n",
        "    groundtruths, predictions,\n",
        "    display_labels=[ewc_label_names[i] for i in np.unique(groundtruths)],\n",
        "    normalize='true',\n",
        "    ax=ax)\n",
        "# note that not all classes are present in the test dataset\n",
        "\n",
        "# rotate x labels for better readability\n",
        "ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSCSfteH5anR"
      },
      "source": [
        "The confusion matrix reveals a few limitations of our trained model:\n",
        "* the model is unable to predict some of the classes (e.g., shrubland, cropland and bare/sparse vegeation; and it is bad at predicting built-up areas)\n",
        "* the model is very good at identifying water surfaces\n",
        "* there is significant confusion between a number of classes (e.g., a variety of classes are confused with tree cover and grassland)\n",
        "\n",
        "**Question**: What are the reasons for these limitations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ8RBdwS5anS"
      },
      "source": [
        "<a id='inference'></a>\n",
        "## 8. Inference\n",
        "\n",
        "Now that our model is trained and evaluated, we can use it to predict the most common land-use/land-cover class in an image patch.\n",
        "\n",
        "Let's pick a random patch from our test dataset and run it through the model to perfom a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2kTP7Ro5anS"
      },
      "outputs": [],
      "source": [
        "i = 42\n",
        "\n",
        "# we retrieve a random sample\n",
        "sample = test_data[i]\n",
        "\n",
        "input = sample['s2']\n",
        "output = model(input.view(1, 4, 120, 120).to(device))  # we have to change the shape of the input\n",
        "prediction = torch.argmax(output, dim=1).reshape(120, 120).cpu().numpy()\n",
        "\n",
        "# display prediction\n",
        "test_data.display(i, pred=prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HSVxsoV6pWN"
      },
      "source": [
        "The model gets the overall shapes right, but it fails to get the right classes in many cases."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "6e741a586f6b68b554925f4bea211b6852c45fbf3b1f1159871bb4b38b6bf4de"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
