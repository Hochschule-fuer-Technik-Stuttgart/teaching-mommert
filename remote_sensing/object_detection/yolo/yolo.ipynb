{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC-dXo_Cqqxx"
   },
   "source": [
    "#  Object Detection with YOLO\n",
    "\n",
    "*HFT Stuttgart, 2025 Summer Term, Michael Mommert (michael.mommert@hft-stuttgart.de)*\n",
    "\n",
    "Object Detection is able to classify instances of objects in an image and to approximately locate these instances. In this Notebook, we use the YOLO model to perform object detection on cars from aerial imagery of the city of Stuttgart. For a streamlined implementation, we use the [ultralytics YOLO framework](https://docs.ultralytics.com/) and fine-tune a pretrained model on our dataset. If you want some more resources on using this framework for training on a custom dataset, you can start [here](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNfWGUHyrBJ5",
    "outputId": "08ab332b-7286-41c4-a957-7abf310c387a"
   },
   "outputs": [],
   "source": [
    "%pip install numpy \\\n",
    "    matplotlib \\\n",
    "    pandas \\\n",
    "    scikit-image \\\n",
    "    ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbUMY_hLfUNu",
    "outputId": "bddf867d-b40b-43ba-9239-8350486fae19"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7GX5bbQaPVw"
   },
   "source": [
    "## Dataset handling\n",
    "\n",
    "We will use the \"Cars in Stuttgart\" dataset in this Notebook. Let's download and unpack the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IU6HbqCr5Jk6",
    "outputId": "bba6d9cb-91c9-41ad-d501-f3f575f3fd15"
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!curl -O https://zenodo.org/records/15019408/files/cars_in_stuttgart.zip\n",
    "\n",
    "# extract dataset zipfile\n",
    "with zipfile.ZipFile('cars_in_stuttgart.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "\n",
    "# rename dataset directory\n",
    "os.rename('cars_in_stuttgart/', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGTRkSOD5Jk6"
   },
   "source": [
    "Let's read in the training image filenames and display one example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "mWwHFYoW5Jk7",
    "outputId": "a06d000b-25b9-40c0-b523-dfa75fcadcea"
   },
   "outputs": [],
   "source": [
    "train_filenames = []\n",
    "for f in sorted(os.listdir('data/train')):\n",
    "    if f.endswith('.png'):  # consider only files ending in .png\n",
    "        train_filenames.append(os.path.join('data/train', f))\n",
    "\n",
    "img = io.imread(train_filenames[2])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7NePyJk5Jk8"
   },
   "source": [
    "For each image file, there is a corresonding label file. For this specific the label file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KBKq3265Jk9"
   },
   "outputs": [],
   "source": [
    "with open(train_filenames[42].replace('.png', '.txt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqBPsgsxaucO"
   },
   "source": [
    "This file uses the YOLO convention to identify **bounding boxes**. Each line corresponds to a different bounding box and the columns have the following meanings:\n",
    "\n",
    "1. Class label: In this case, there's only a single class, so all bounding boxes use label 0. If there were different classes, there would be different ids.\n",
    "2. X center of the bounding box\n",
    "3. Y center of the bounding box\n",
    "4. Width of the bounding box\n",
    "5. Height of the bounding box\n",
    "\n",
    "Parameter 2-5 are given in normalized pixel coordinates, not pixels. For instance, a coordinate 0.8 corresponds to pixel 80 in a 100 pixel image or pixel 40 in a 50 pixel image.\n",
    "\n",
    "Let's read in the bounding box information and plot the boxes on the image. Feel free to check out other image indices as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "G9rmqtkVrI3y",
    "outputId": "0521bdbd-8680-4c83-c373-83c03908459b"
   },
   "outputs": [],
   "source": [
    "i = 2 # image index\n",
    "\n",
    "# read in bounding boxes\n",
    "bbs = pd.read_csv(train_filenames[i].replace('.png', '.txt'), sep=' ',\n",
    "                  header=None, names=['class', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "# plot image\n",
    "img = io.imread(train_filenames[i])\n",
    "f, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img)\n",
    "\n",
    "# add bounding boxes\n",
    "h, w, c = img.shape\n",
    "for i, bb in bbs.iterrows():\n",
    "    # draw a rectangle for each bounding box; rectangle expects top left corner coordinates, width and heigth\n",
    "    ax.add_patch(Rectangle(((bb.x-bb.w/2)*w, (bb.y-bb.h/2)*h),\n",
    "                           bb.w*w, bb.h*h,\n",
    "                 edgecolor='red', facecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUFffG_rabte"
   },
   "source": [
    "## Training Process\n",
    "\n",
    "The training process is extremely convenient using the YOLO framework. To setup the training process, we have to create a **dataset configuration file** that contains information on the following things:\n",
    "* the path to the dataset root directory\n",
    "* the names of the different dataset splits (those should be directories under the root path)\n",
    "* the class labels (remember that we only have a single class in this example)\n",
    "\n",
    "We write this file using some cell magic. Note that you might have to adjust the value of `path:` to be in accordance with your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAgBEPPtb4cE",
    "outputId": "e7adfd26-b3df-42ab-abe1-6090b2711aa4"
   },
   "outputs": [],
   "source": [
    "%%writefile dataset.yaml\n",
    "\n",
    "# Train/val/test sets\n",
    "path: ../data # dataset root dir (Colab: use /content/data, bwJupyter: ../data)\n",
    "train: train # train images (relative to 'path')\n",
    "val: val # val images (relative to 'path')\n",
    "\n",
    "# define classes\n",
    "names:\n",
    "    0: car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6UIUtE0dDan"
   },
   "source": [
    "Now we have to **choose an appropriate model**. The YOLO architecture comes in different [sizes](https://github.com/ultralytics/ultralytics). The term size relates here to the number of learnable parameters that this model has. Typically, the more parameters, the more powerful the fully trained model.\n",
    "\n",
    "We will use the smallest available model, `yolo11n`, with 2.6 million parameters and load a pretrained version of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZageoXiaRqN-",
    "outputId": "ff348be6-1ae5-49b5-8354-1f038920bdb4"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXmMyJ3LRpT9"
   },
   "source": [
    "Now we can stat the actual **model training or fine-tuning process**. Naturally, this process is complex and comes with a lot of parameters that can be set. We will set some of these parameters below and use the default parameters for the rest of them. For a full discussion of all available parameters, please visit the [ultralytics YOLO documentation](https://docs.ultralytics.com/usage/cfg/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQbUaSttc4x5",
    "outputId": "9639d7dd-6900-4903-eae2-c02e8784121c"
   },
   "outputs": [],
   "source": [
    "results = model.train(data='dataset.yaml', # define the dataset configuration file\n",
    "                      task='detect', # set the task as object detection\n",
    "                      epochs=15, # set the number of training epochs\n",
    "                      seed=42, # set a fixed seed value for reproducibility\n",
    "                      imgsz=128, # provide the size of the images\n",
    "                      batch=16, # batch size 16 images\n",
    "                      optimizer='Adam', # use the Adam optimizer\n",
    "                      lr0=0.001, # learning rate at epoch 0\n",
    "                      plots=True, # plot analytics during training\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ff-EHqulhyV"
   },
   "source": [
    "The training process generates a lot of output, which includes all the parameters and seetings (including the default ones we did not touch), data disgnostics (e.g., corrupt bounding boxes) and, of course, information on the training progress.\n",
    "\n",
    "In addition, YOLO creates a lot of files in a new directory called `runs/`. Each training run gets a new directory that are labeled accordingly: `train`, `train1`, `train2`... Here you can find diagnostic plots and logfiles, example predictions and the trained model checkpoints. You might have to adjust the path in the following code cell to pick the right training run.\n",
    "\n",
    "You will also find a plot named `results.png` that summarizes the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "CEqOUFq0oqnI",
    "outputId": "41b3ca2e-454a-4fed-9c50-f2d3d97a52d9"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12,6))\n",
    "ax.imshow(io.imread(\"runs/detect/train/results.png\"))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxCqP7espVM9"
   },
   "source": [
    "There's quite a number of plots here. Let's see what we have...\n",
    "\n",
    "* The **box loss** describes the value of the loss function for localizing bounding boxes. The plot shows the box loss separately for the train and val datasets.\n",
    "* The **cls loss** describes the value of the loss function for identifying the correct classes of the bounding boxes we found. There is a separate cls loss for the train and the val loss.\n",
    "* The **dfl loss** is the distributed focal loss, which focuses on detecting difficult-to-find objects. Again, there is one for the train dataset and one for the val dataset.\n",
    "* We also have the precision metric,\n",
    "* the recall metric,\n",
    "* the mean Average Precision 50 (mAP50) and 50-95 (mAP50-95) metrics.\n",
    "\n",
    "All plots indicate a good learning progress. Let's perform an evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Iv4AL_VHHP7"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate our model on the test dataset, we simply create a new dataset configuration file, but provide the test dataset as our validation dataset (you might have to adjust `path:` again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOn_vR6fImy6",
    "outputId": "19ff9d4e-aea6-45fb-9715-71bb44751359"
   },
   "outputs": [],
   "source": [
    "%%writefile dataset_test.yaml\n",
    "\n",
    "# Train/val/test sets\n",
    "path: ../data # dataset root dir (Colab: use /content/data, bwJupyter: ../data)\n",
    "train: train # train images (relative to 'path')\n",
    "val: test # this time we use the test dataset for evaluations (relative to 'path')\n",
    "\n",
    "# define classes\n",
    "names:\n",
    "    0: car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzmoYCbWIziJ"
   },
   "source": [
    "Now we simply rerun the validation step (but this time it will utilize the test dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQiDav0DI726",
    "outputId": "fa778b43-bf6d-4967-c9c4-c39915fdcf1c"
   },
   "outputs": [],
   "source": [
    "test_results = model.val(data=\"dataset_test.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNIot_W6LUq4"
   },
   "source": [
    "The results of the evaluation are written to files. Another way to access them is through the output of the `val` method. Let's have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p25v7RIELK2w",
    "outputId": "74e6bb4b-1f0c-4131-b2db-1798406a4e3b"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 2, figsize=(15,15))\n",
    "ax = np.ravel(ax)\n",
    "\n",
    "for i in range(len(test_results.curves)):\n",
    "    ax[i].plot(test_results.curves_results[i][0], test_results.curves_results[i][1][0])\n",
    "    ax[i].set_xlabel(test_results.curves_results[i][2])\n",
    "    ax[i].set_ylabel(test_results.curves_results[i][3])\n",
    "    ax[i].set_title(test_results.curves[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4pzf4_9LDBo",
    "outputId": "f02aa191-4e5b-4c2f-da05-e19d1b169bd9"
   },
   "outputs": [],
   "source": [
    "for metric, value in test_results.results_dict.items():\n",
    "    print(metric, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8Q4Z-FuB514"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's perform inference for one image from our training dataset. In the end, we simply provide a list of filenames to our model, which will return a `result` object for each sample. For a full discussion of the prediction step, please review the [predict guide](https://docs.ultralytics.com/modes/predict/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "A4qFcRYkMQM5",
    "outputId": "0c045169-ca20-4d6b-a7f4-9a6a9d0bbd1d"
   },
   "outputs": [],
   "source": [
    "i = 2 # image index\n",
    "\n",
    "# Perform inference on one image\n",
    "results = model([train_filenames[i]],\n",
    "                conf=0.4, # detection confidence threshold\n",
    "                iou=0.6, # detection iou threshold)\n",
    "               )\n",
    "\n",
    "# extract bounding boxes and confidences\n",
    "bbs = results[0].boxes.xywh.cpu().numpy()\n",
    "confs = results[0].boxes.conf.cpu().numpy()\n",
    "\n",
    "# plot image\n",
    "img = io.imread(train_filenames[i])\n",
    "f, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img)\n",
    "\n",
    "# add bounding boxes to plot\n",
    "for i in range(bbs.shape[0]):\n",
    "    # draw a rectangle for each bounding box; rectangle expects top left corner coordinates, width and heigth\n",
    "    ax.add_patch(Rectangle(((bbs[i][0]-bbs[i][2]/2), (bbs[i][1]-bbs[i][3]/2)),\n",
    "                           bbs[i][2], bbs[i][3], edgecolor='yellow', facecolor='none'))\n",
    "    # add confidence values\n",
    "    ax.annotate('{:.1f}%'.format(confs[i]*100), xy=(bbs[i][0]-bbs[i][2]*0.3, bbs[i][1]), fontsize=12, color='yellow')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
