{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d149aa",
   "metadata": {},
   "source": [
    "# Copernicus Data Access with openEO\n",
    "\n",
    "Michael Mommert, Stuttgart University of Applied Sciences, 2025\n",
    "\n",
    "This Notebook introduces the openEO client library for Python. [OpenEO](https://openeo.org) provides a common API to easily connect to Earth observation data backends. This makes it significantly easier to access a wide range of data products from different products. This Notebook is based on the [official quickstart guide](https://openeo.org/documentation/1.0/python) and [another example Notebook](https://documentation.dataspace.copernicus.eu/notebook-samples/openeo/NDVI_Timeseries.html).\n",
    "\n",
    "In this tutorial, we will focus on the Copernicus Data Space Ecosystem (CDSE) as our backend; if you need more information on this, you can refer to the [Copernicus Data Space Ecosystem Documentation Portal](https://documentation.dataspace.copernicus.eu/).\n",
    "\n",
    "Before we can use openEO, we have to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1439c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib pandas geopandas rasterio openeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffe25b",
   "metadata": {},
   "source": [
    "Now we can import the openEO module and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import rasterio\n",
    "import openeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894afa9c",
   "metadata": {},
   "source": [
    "## Backends\n",
    "\n",
    "A wide range of backends are available for use with openEO. Those include the Copernicus programme, the Google Earth Engine and others. A full list is available through the [open EO Hub](https://hub.openeo.org/).\n",
    "\n",
    "We will now explore the Copernicus backend. The first step is to establish a connection to the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = openeo.connect(\"https://openeo.dataspace.copernicus.eu/openeo/1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203908ed",
   "metadata": {},
   "source": [
    "Now we can explore the available collections, each of which is a dataset provided to the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33222db5",
   "metadata": {},
   "source": [
    "As you can see, a huge variety of datasets across different data modalities and data products are available. We can explore the individual collections interactively within this Jupyter Notebook.\n",
    "\n",
    "In the following, we will focus on the Sentinel-2 Level-2A data collection.\n",
    "\n",
    "## Authentication\n",
    "\n",
    "Before we can access data from this collection, we have to authenticate. In the case of the Copernicus Data System Ecosystem (CDSE), this is done using OpenID through a web browser. Note that you need to be registered with the CDSE (or whatever backend you plan to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2abb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107980d",
   "metadata": {},
   "source": [
    "Now that we are registered, we can access the data collection, which is done via datacubes.\n",
    "\n",
    "## Datacubes\n",
    "\n",
    "A [datacube](https://openeo.org/documentation/1.0/datacubes.html#what-are-datacubes) defines a slice through a specific data collection. Consider the case that we are interested in accessing Sentinel-2 data for a specific area taken in June of 2025. We can create the corresponding datacube as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe92e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube = connection.load_collection(\n",
    "  \"SENTINEL2_L2A\",\n",
    "  spatial_extent={\"west\": 9.253, \"south\": 48.698, \"east\": 9.275, \"north\": 48.710},\n",
    "  temporal_extent=[\"2025-06-15\", \"2025-06-30\"],  # start and end date\n",
    "  max_cloud_cover=20,  # permitted cloud cover percentage\n",
    "  bands=[\"B04\", \"B03\", \"B02\", \"B08\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986d0ba",
   "metadata": {},
   "source": [
    "As you can see, we can define the spatial extent, the temporal extent, the maximum cloud cover and the bands that we are interested. For a full list of features, please refer to the corresponding [documentation](https://open-eo.github.io/openeo-python-client/api.html#openeo.rest.datacube.DataCube.load_collection). The datacube per se does nothing. We could, in theory apply a process to this datacube, which processes the data (aggregation, filtering etc). In this example, we will not do this, but you find additional information on how to do this [here](https://openeo.org/documentation/1.0/python/#applying-processes).\n",
    "\n",
    "Instead, we will access the information and download them into our Jupyter environment. Note that this will take a few minutes. Your request will not be immediately processed. Instead, your request will be queued and processed when the required resources are available. This is called batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define how we want to save the data\n",
    "result = datacube.save_result(\"GTiff\")\n",
    "\n",
    "# we create a new job at the backend\n",
    "job = result.create_job(validate=True)\n",
    "\n",
    "# now we start the job and wait for the data to download into a designated directory\n",
    "job.start_and_wait()\n",
    "data_dir = 'sen2_data'\n",
    "job.get_results().download_files(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af27eb",
   "metadata": {},
   "source": [
    "The data will be written to file in the directory that we can provided (if the directory does not yet exist, a new one will be created).\n",
    "\n",
    "We can now open and display the downloaded images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a004e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile a list of downloaded images\n",
    "filelist = []\n",
    "for filename in sorted(os.listdir(data_dir)):\n",
    "    if filename.endswith('tif'):\n",
    "        filelist.append(os.path.join(data_dir, filename))\n",
    "\n",
    "n_rows = int(np.sqrt(len(filelist)))\n",
    "n_cols = int(len(filelist)/n_rows)\n",
    "\n",
    "f, ax = plt.subplots(n_rows, n_cols, figsize=(n_rows*5, n_cols*5))\n",
    "ax = np.ravel(ax) # flatten ax array\n",
    "\n",
    "for i in range(len(filelist)):\n",
    "    # read in image data\n",
    "    dataset = rasterio.open(filelist[i])\n",
    "    rgb = np.dstack(dataset.read((1,2,3)))  # trim NIR band and rearrange data\n",
    "\n",
    "    # normalize rgb data\n",
    "    rgb = (rgb - np.percentile(rgb, 1))/(np.percentile(rgb, 99)-np.percentile(rgb, 1))\n",
    "\n",
    "    # plot data\n",
    "    ax[i].imshow(rgb)\n",
    "    ax[i].set_title(filelist[i])\n",
    "    ax[i].axis('off')\n",
    "\n",
    "f.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf37a9d",
   "metadata": {},
   "source": [
    "**Exercise**: Define a datacube yourself and download the corresponding data. Choose a small area (e.g., using services like [geojson.io](https://geojson.io/)) and a short period of time to keep the download small. If you run into errors, carefully read the error message and fix your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc89e7",
   "metadata": {},
   "source": [
    "## Applying Processes to Areas of Interest\n",
    "\n",
    "Instead of downloading image crops and analysing those locally on your computer, you can also query individual areas of interest (e.g., polygons) and analyse those directly on the backend computer. This has the advantage that you don't have to download large amounts of data; instead you bring your analysis to the computer where the data is stored.\n",
    "\n",
    "Let's begin by defining some areas of interest. In our case, we will define some polygons based on the images above. These polygons are stored in a geojson file. Let's read in this geojson file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download geojson file\n",
    "!wget https://github.com/Hochschule-fuer-Technik-Stuttgart/teaching-mommert/blob/main/remotesensing/openeo/aoi.geojson?raw=true -O aoi.geojson\n",
    "\n",
    "# read in polygons\n",
    "polygons = gpd.read_file('aoi.geojson')\n",
    "polygons.set_crs(epsg=4326)  # set a coordinate reference system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc89e",
   "metadata": {},
   "source": [
    "Let's visualize these polygons on the images that we already downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images\n",
    "n_rows = int(np.sqrt(len(filelist)))\n",
    "n_cols = int(len(filelist)/n_rows)\n",
    "\n",
    "f, ax = plt.subplots(n_rows, n_cols, figsize=(n_rows*5, n_cols*5))\n",
    "ax = np.ravel(ax) # flatten ax array\n",
    "\n",
    "for i in range(len(filelist)):\n",
    "    # read in image data\n",
    "    dataset = rasterio.open(filelist[i])\n",
    "    rgb = np.dstack(dataset.read((1,2,3)))  # trim NIR band and rearrange data\n",
    "    crs = dataset.crs  # extract crs \n",
    "    transform = dataset.transform  # extract transform\n",
    "\n",
    "    # normalize rgb data\n",
    "    rgb = (rgb - np.percentile(rgb, 1))/(np.percentile(rgb, 99)-np.percentile(rgb, 1))\n",
    "\n",
    "    # plot data\n",
    "    ax[i].imshow(rgb)\n",
    "    ax[i].axis('off')\n",
    "\n",
    "    # add polygons to plot\n",
    "    local_polygons = polygons.to_crs(crs)  # convert polygons to local crs\n",
    "    for geometry in local_polygons.geometry:\n",
    "        if geometry.geom_type == 'Polygon':\n",
    "            x, y = geometry.exterior.xy  # extract node coordinates in local crs\n",
    "            pixels = np.array([~transform * (xi, yi) for xi, yi in zip(x, y)])  # convert node coordinates to image coordinates using inverse transform\n",
    "            ax[i].plot(pixels[:, 0], pixels[:, 1], color='red', linewidth=1)  # plot polygons\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdf757",
   "metadata": {},
   "source": [
    "For these areas of interest, we would now like to compute the mean ndvi values for each observation over a full year. We will do this using processes, which run on the backend. This means that we can compute these ndvi values without having to download the actual satellite imagery.\n",
    "\n",
    "First, we have to define another datacube. This datacube will not include information on the spatial extent. This information will come directly from the polygons, which we defined in the geojson file. Note that we are using the lowest possible maximum cloud cover of 1% here to only query data without cloud contaminations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoicube = connection.load_collection(\n",
    "  \"SENTINEL2_L2A\",\n",
    "  temporal_extent=[\"2024-10-01\", \"2025-09-30\"], #, \"2024-11-29\", \"2024-12-26\", \"2025-02-17\", \"2025-03-18\", \"2025-04-10\", \n",
    "                    #\"2025-05-10\", \"2025-06-19\", \"2025-07-02\", \"2025-08-13\", \"2025-09-20\"]], # a list of more than two dates indicates a sequence\n",
    "  max_cloud_cover=1,  # permitted cloud cover percentage\n",
    "  bands=[\"B04\", \"B08\"]  # only retrieve red and nir bands\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d17b3",
   "metadata": {},
   "source": [
    "Now we can define the individual bands and the band arithmetic to compute ndvi. Finally, we create a process to aggregate the data spatially, based on our polygons and using the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = aoicube.band(\"B04\")\n",
    "nir = aoicube.band(\"B08\")\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "\n",
    "# read areas of interest as json\n",
    "with open('aoi.geojson', 'r') as inf:\n",
    "    aoi_json = json.load(inf)\n",
    "\n",
    "# define batch process\n",
    "ndvi_timeseries = ndvi.aggregate_spatial(geometries=aoi_json, reducer=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579de550",
   "metadata": {},
   "source": [
    "Just like we did before, we can now send the process to the backend to get processed. We will wait for the results and write them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute job\n",
    "job = ndvi_timeseries.execute_batch(out_format=\"CSV\", title=\"NDVI timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d09ee",
   "metadata": {},
   "source": [
    "Once the job finished, we can download the results and read them into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20927018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download results and read them in as a Pandas Dataframe\n",
    "job.get_results().download_file(\"ndvi/ndvi_timeseries.csv\")\n",
    "df = pd.read_csv(\"ndvi/ndvi_timeseries.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea7202",
   "metadata": {},
   "source": [
    "We plot the results, one line per mean ndvi for each polygon, as a function of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_filename = \"ndvi/ndvi_timeseries.csv\"\n",
    "ndvi_df = pd.read_csv(ndvi_filename, index_col=0).sort_index()\n",
    "ndvi_df.index = pd.to_datetime(ndvi_df.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ndvi_df.groupby(\"feature_index\")[\"band_unnamed\"].plot(marker=\"o\", ax=ax)\n",
    "ax.set_title(\"NDVI timeseries\")\n",
    "ax.set_ylabel(\"NDVI\")\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f933a5",
   "metadata": {},
   "source": [
    "We find lots of variations, but also some systematics: which line corresponds to which polygon?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0991df6",
   "metadata": {},
   "source": [
    "**Exercise**: Define your own polygons using [https://geojson.io](https://geojson.io) and download aggregated ndvi values. Pick some fields in a subtropical country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
