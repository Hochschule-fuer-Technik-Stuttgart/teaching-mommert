{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deb7a20-b5ea-421a-9232-db136c2c6ddb",
   "metadata": {},
   "source": [
    "# Regression on the California Housing Dataset\n",
    "\n",
    "Michael Mommert, Stuttgart University of Applied Sciences, 2024\n",
    "\n",
    "This Notebook introduces regression as a task. We will use the [California Housing dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html), which contains a number of numerical features, such as the number of rooms, the total population and median income of the neighborhood, as well as geographical location. We will implement and test a number of different traditional Machine Learning models to predict the *median house value* per district based on a number of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fe4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy \\\n",
    "    scipy \\\n",
    "    pandas \\\n",
    "    matplotlib \\\n",
    "    scikit-learn \\\n",
    "    seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447fdd5-1184-4890-a2d5-069c297a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a64e25-f21c-4321-8771-d93dc9cbe1fc",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "The dataset is easily accessible through the SciKit-Learn module. The module provides functionality to download the dataset and read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448cbf0-8a58-49bf-b119-82eaae224720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137d10c-9eb9-4737-aed2-278e32fbb73e",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before we use the dataset, let's do some data exploration to better get to know the dataset. Let's see what `data` contains (`data` is actually a dictionary, so we can check its keys to learn more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47489224-7945-4c45-88f4-ee7b6d013ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e090c-c31b-4fcd-ba56-3dc32bf05d4c",
   "metadata": {},
   "source": [
    "Conveniently, `data` contains a full description of the dataset. Let's consult that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf71f65-d5fc-48f1-a764-118de7d4e75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c828cc9-19be-4c5f-b83d-a3719960305f",
   "metadata": {},
   "source": [
    "This is very interesting. Before we move on, let's convert the data into a more useful form\n",
    "\n",
    "First, we store the actual data into an array, which we call `X`. `X` will serve as the input data array with a shape of (20640, 8), 20640 samples with 8 features, each. Correspondingly, we create an array `y` that contains the target (the median house value per district).\n",
    "\n",
    "Second, we turn `X` into a Pandas dataframe for better human readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "y = data['target']\n",
    "df = pd.DataFrame(X, columns=data['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b64948",
   "metadata": {},
   "source": [
    "Now, let's have a look at some numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5766843-155c-4349-a80e-985bf734a7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f910870-6962-4869-bcc1-a61644a5971a",
   "metadata": {},
   "source": [
    "It is obvious that many columns in this table are subject to significant outliers. This is something to keep in mind.\n",
    "\n",
    "Let's visualize some data. We will plot the distribution of datapoints as a function their geographical longitude and latitude. To add more value to the plot, we will color the data points based on their *median house value*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457c142-c1f6-4eb3-978b-76f96621265c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = plt.scatter(X[:, 7], X[:, 6], c=y, alpha=0.3, edgecolor='none')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.colorbar(p, label='median house value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd76ea2-df37-4e7d-9fb1-ca963c34b9b8",
   "metadata": {},
   "source": [
    "This looks familiar... What do you see?\n",
    "\n",
    "**Exercise**: Repurpose the map above to show the *population* per district as colored dots. Can you identify and name the most densely populated district?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d42c43-9c64-45c1-9011-388ce5907b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e971f0-22cc-4fb8-9bc9-fc641ace2093",
   "metadata": {},
   "source": [
    "To explore correlations between the individual features and the target, we can use a pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e9ad-ef2e-499b-bd41-7cb286d1d6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seaborn import pairplot\n",
    "\n",
    "df = pd.DataFrame(X, columns=data['feature_names'])\n",
    "df = pd.concat([df, pd.DataFrame(y, columns=data['target_names'])], axis=1)\n",
    "\n",
    "pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3fc50-f790-4761-a515-8f6fad1f2c4e",
   "metadata": {},
   "source": [
    "There seem to be correlations between some of the features and the target quantity (*Median House Value*). Let's quantify these correlations using the **Pearson Correlation Coefficient**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b546f85-8f78-476b-8cd5-03d96e7d8223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr_results = []\n",
    "for i in range(X.shape[1]):\n",
    "    pearsonr_results.append(pearsonr(X[:, i], y))\n",
    "\n",
    "pearsonr_df = pd.DataFrame({\n",
    "    'feature': data['feature_names'],\n",
    "    'r': [r.statistic for r in pearsonr_results],\n",
    "    'pvalue': [r.pvalue for r in pearsonr_results]})\n",
    "pearsonr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec87d1b8-3c8a-42ae-ad8e-e0cd98e0efd4",
   "metadata": {},
   "source": [
    "The [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) provides a means to estimate the linear correlation between two variables. (The following introduction is provided by the [SciPy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html).)\n",
    "\n",
    "    The Pearson correlation coefficient measures the linear relationship between two datasets. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact linear relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "    This function also performs a test of the null hypothesis that the distributions underlying the samples are uncorrelated and normally distributed. The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets.\n",
    "\n",
    "What does this analysis tell us? A few variables (like the median income, the house age, the average number of rooms and the geographic latitude) seem to be correlated to the median house value. These might be good candidates for using them in linear models for predicting the median house value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5b164-7146-43fb-b474-c22a9ec2a074",
   "metadata": {},
   "source": [
    "**Exercise**:  Plot the *median house value* as a function of the *median income* per district. What do you observe? Is the correlation linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6777b8-1a87-4f82-b46d-167d409f2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee8cb2-4f10-430f-ae02-ae8d4d032aba",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before we start the modeling process, we have to prepare the dataset. This includes the **splitting** of the dataset into a *train*, *validation* and *test* split and the subsequent **scaling** (or normalization) of each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e492768-5018-4b1b-9d56-fcf2f1517cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(X, y, shuffle=True, train_size=0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remain, y_remain, shuffle=True, train_size=0.5)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8689cb-9ae5-4b40-8203-d0831fbb6e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed1082-085f-4c08-a4c7-8a74e8a55d84",
   "metadata": {},
   "source": [
    "Now that the dataset is prepared, we can start the modeling using different regression approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f1ea8-efbd-4979-bf47-3f42347194d2",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We will start with a simple linear regression model. Based on our earlier findings, we will only use the input variables median income (index 0), the house age (1), the average number of rooms (2) and the geographic latitude (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81d217-30c9-4bb7-b489-0c1b44ccce93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train[:, [0, 1, 2, 6]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0fca3-c605-4391-9da7-f705af241435",
   "metadata": {},
   "source": [
    "We evaluate the model on all three data splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf52935-72d8-416f-842d-ed07a466632a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train split\n",
    "pred = model.predict(X_train[:, [0, 1, 2, 6]])\n",
    "print('train', np.sqrt(mean_squared_error(y_train, pred)))\n",
    "\n",
    "# validation split\n",
    "pred = model.predict(X_val[:, [0, 1, 2, 6]])\n",
    "print('val', np.sqrt(mean_squared_error(y_val, pred)))\n",
    "\n",
    "# train split\n",
    "pred = model.predict(X_test[:, [0, 1, 2, 6]])\n",
    "print('test', np.sqrt(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d62dbd-7610-4ad2-84d5-8b841f82dfb6",
   "metadata": {},
   "source": [
    "The performance across all three datasets is very similar. Therefore, we can confidently rule out overfitting in this case.\n",
    "\n",
    "By studying the trained model's coefficients, we can get a feeling for which of the input variables are most important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1b548-df88-437a-a4bf-057c847c660e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(zip(['MedInc', 'HouseAge', 'AveRooms', 'Latitude'], model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a8ef4-57b9-409c-bd53-8f60811d269e",
   "metadata": {},
   "source": [
    "**Exercise**: Implement a linear model that uses all available input variables. How do the model's coefficients change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fac3c-f285-4b44-a40c-233a8df4f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e411ed9-db81-45b6-9760-9e012f208465",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "[LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) (least absolute shrinkage and selection operator) consists of a linear model in combination with L1-regularization. The degree of the regularization is controlled via the $\\alpha$ parameter. Strong regularization leads to the elimination of some of the input variables by setting their weight coefficients to zero. Therefore, LASSO can be used as a method to identify the most import input variables. Let's apply LASSO to our full input dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b716b16-5a6a-467a-a07c-7e3303e430f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "for alpha in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))\n",
    "    pred_val = model.predict(X_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "    print('alpha: {:.1e}, rmse_train: {:.3f}, rmse_val: {:.3f}, \\ncoeffs: {:s}'.format(alpha, rmse_train, rmse_val, str(model.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376eb2a-1d3a-41e9-9f94-0988e14275e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Two things are noteworthy here:\n",
    "\n",
    "* the higher alpha, the smaller the gap between the training and validation datasets: this indicates that higher values of alpha lead to stronger regularization\n",
    "* the higher alpha, the larger then number of model weight coefficients that are set to zero: regularization leads to the elimination of input variables\n",
    "\n",
    "**Exercise**: Compare the list of remaining input variables for the highest value of alpha with the list of correlation coefficients that we derived above. Are the remaining variables those with the highest correlation coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69506dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930c3eb-4046-4243-b3cc-834d00a7a260",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbor Regression\n",
    "\n",
    "We will now use a $k$ nearest neighbor model for our regression task. Since $k$ is the method's hyperparameter, we will train the model for different values of $k$ and evaluate on the training and validation datasets to identify overfitting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c83cb-8b23-4a53-9a32-56c8687d36d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for n in [1, 5, 10, 20, 50, 100, 1000]:\n",
    "    model = KNeighborsRegressor(n_neighbors=n)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, pred))\n",
    "        \n",
    "    pred = model.predict(X_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "    print('n: {:d}, rmse_train: {:.3f}, rmse_val: {:.3f}'.format(n, rmse_train, rmse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27421893-7c51-47a0-93f3-d7b148e5bdf5",
   "metadata": {},
   "source": [
    "There are a few observations here:\n",
    "* the RMSE on the training data is zero by definition, since we derive the value of each sample from the training dataset based on its closest member in the training dataset - this is, of course, the exact same sample; therefore, the RMSE is zero\n",
    "* the RMSE on the validation dataset starts high for small values of $k$ and then decreases before it increases again; this is a result of overfitting for small $k$ and underfitting for large $k$\n",
    "* the discrepancy between the training RMSE and the validation RMSE is a result of overfitting; this discrepancy is highest for small $k$, a clear indicator of overfitting\n",
    "\n",
    "**Exercise**: Which value of $k$ would you adopt and why? Retrain the model for this value of $k$ and evaluate the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097765f4-1b28-4284-8e1e-8a71f2891466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5a64c-1947-4eb3-9755-46431393d675",
   "metadata": {},
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "Finally, we will train a random forest model for regression. Random forests are rather powerful and have a number of hyperparameters. For now, we will only consider the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaf9f6-f3f9-4cbd-a3d6-769e649e8589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for n in [1, 50, 100]:\n",
    "    model = RandomForestRegressor(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))\n",
    "    \n",
    "    pred_val = model.predict(X_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "    print('n: {:d}, rmse_train: {:.3f}, rmse_val: {:.3f}'.format(n, rmse_train, rmse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b349264-f294-44ff-9aa7-1f8e0a7f45f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "The evaluation results are strongly affected by overfitting.\n",
    "\n",
    "**Exercise**: Explore ways to regularize this model based on the [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Evaluate your model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fe7b9-b646-4d17-a1fa-499ec0f81c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db02c6a-dad2-4cef-adf2-2bf8ca87757a",
   "metadata": {},
   "source": [
    "One big advantage of random forests (or decision trees in general) is their ability to produce *feature importances*, indicating how useful or important each input feature is for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64f19c-1475-47c2-8a08-9f5ad3940fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(zip(data['feature_names'], model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c369d-508d-4061-a1bc-bf9117f6b462",
   "metadata": {},
   "source": [
    "**Exercise**: Compare these feature importances with the results from our Spearman Rank correlation and LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835cd14-ff7f-4b6d-a50e-a68eb63e547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for the exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
